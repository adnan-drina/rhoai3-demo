# =============================================================================
# Step 05: LLM Model Registration - S3 Registry Handshake
# =============================================================================
# Registers models with S3 URIs in the Model Registry.
# The Registry stores the "Blueprint" (metadata + S3 pointer), not the weights.
#
# RHOAI 3.0 Handshake Pattern:
#   Registry (Metadata) → MinIO (Weights) → KServe (Execution)
#
# When users click "Deploy" in GenAI Studio, RHOAI:
#   1. Reads the 'uri' field from the ModelArtifact
#   2. Auto-fills the storageUri in the InferenceService
#   3. Uses the Data Connection secret for credentials
#
# Ref: https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0/html-single/working_with_model_registries/index
# =============================================================================
apiVersion: batch/v1
kind: Job
metadata:
  name: llm-model-registration
  namespace: rhoai-model-registries
  labels:
    app: llm-model-registration
  annotations:
    argocd.argoproj.io/sync-wave: "1"
    argocd.argoproj.io/hook: Sync
    argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
spec:
  backoffLimit: 5
  ttlSecondsAfterFinished: 300
  template:
    metadata:
      labels:
        app: llm-model-registration
    spec:
      restartPolicy: OnFailure
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      containers:
        - name: register
          image: quay.io/curl/curl:latest
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop: ["ALL"]
          env:
            - name: REGISTRY_URL
              value: "http://private-ai-registry-internal.rhoai-model-registries.svc:8080"
            # S3 Configuration for MinIO
            - name: S3_ENDPOINT
              value: "http://minio.minio-storage.svc:9000"
            - name: S3_BUCKET
              value: "models"
          command:
            - /bin/sh
            - -c
            - |
              set -e
              echo "╔══════════════════════════════════════════════════════════════╗"
              echo "║  Step 05: LLM Model Registration (S3 Handshake)              ║"
              echo "║  Registry stores metadata → MinIO stores weights             ║"
              echo "╚══════════════════════════════════════════════════════════════╝"
              
              API_BASE="${REGISTRY_URL}/api/model_registry/v1alpha3"
              
              echo "Waiting for Model Registry API..."
              for i in $(seq 1 30); do
                if curl -sf "${API_BASE}/registered_models" > /dev/null 2>&1; then
                  echo "✓ Model Registry API is ready!"
                  break
                fi
                echo "  Attempt $i/30 - waiting..."
                sleep 10
              done
              
              # ═══════════════════════════════════════════════════════════════════
              # Function to register a model with S3 URI
              # ═══════════════════════════════════════════════════════════════════
              register_model() {
                local MODEL_NAME="$1"
                local MODEL_DESC="$2"
                local MODEL_TAGS="$3"
                local VERSION_NAME="$4"
                local VERSION_META="$5"
                local S3_PATH="$6"
                local SOURCE_NAME="$7"
                local MODEL_FORMAT="$8"
                
                # Construct full S3 URI
                local ARTIFACT_URI="s3://${S3_BUCKET}/${S3_PATH}"
                
                echo ""
                echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
                echo "Registering: ${MODEL_NAME}"
                echo "  S3 URI: ${ARTIFACT_URI}"
                echo "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
                
                if curl -sf "${API_BASE}/registered_models" | grep -q "\"name\":\"${MODEL_NAME}\""; then
                  echo "✓ Model '${MODEL_NAME}' already exists - skipping"
                  return 0
                fi
                
                # Create RegisteredModel
                MODEL_RESP=$(curl -sf -X POST "${API_BASE}/registered_models" \
                  -H "Content-Type: application/json" \
                  -d "{
                    \"name\": \"${MODEL_NAME}\",
                    \"description\": \"${MODEL_DESC}\",
                    \"owner\": \"ai-admin\",
                    \"customProperties\": ${MODEL_TAGS}
                  }")
                MODEL_ID=$(echo "$MODEL_RESP" | sed 's/.*"id":"\([^"]*\)".*/\1/' | head -1)
                echo "  Created RegisteredModel ID: ${MODEL_ID}"
                
                # Create ModelVersion
                VERSION_RESP=$(curl -sf -X POST "${API_BASE}/model_versions" \
                  -H "Content-Type: application/json" \
                  -d "{
                    \"name\": \"${VERSION_NAME}\",
                    \"description\": \"Production-ready version for RHOAI 3.0\",
                    \"author\": \"ai-admin\",
                    \"registeredModelId\": \"${MODEL_ID}\",
                    \"customProperties\": ${VERSION_META}
                  }")
                VERSION_ID=$(echo "$VERSION_RESP" | sed 's/.*"id":"\([^"]*\)".*/\1/' | head -1)
                echo "  Created ModelVersion ID: ${VERSION_ID}"
                
                # Create ModelArtifact with S3 metadata (Official RHOAI 3.0 format)
                # This enables the "Deploy from Registry" flow in GenAI Studio
                curl -sf -X POST "${API_BASE}/model_versions/${VERSION_ID}/artifacts" \
                  -H "Content-Type: application/json" \
                  -d "{
                    \"name\": \"${VERSION_NAME}\",
                    \"uri\": \"${ARTIFACT_URI}\",
                    \"artifactType\": \"model-artifact\",
                    \"modelFormatName\": \"${MODEL_FORMAT}\",
                    \"customProperties\": {
                      \"s3_endpoint\": {\"metadataType\": \"MetadataStringValue\", \"string_value\": \"${S3_ENDPOINT}\"},
                      \"s3_bucket\": {\"metadataType\": \"MetadataStringValue\", \"string_value\": \"${S3_BUCKET}\"},
                      \"format\": {\"metadataType\": \"MetadataStringValue\", \"string_value\": \"safetensors\"},
                      \"source_model\": {\"metadataType\": \"MetadataStringValue\", \"string_value\": \"${SOURCE_NAME}\"}
                    }
                  }" > /dev/null
                
                echo "✓ Registered: ${MODEL_NAME} → ${ARTIFACT_URI}"
              }
              
              # ═══════════════════════════════════════════════════════════════════
              # Model 1: Mistral Small 24B BF16 (4-GPU, Full Precision)
              # ═══════════════════════════════════════════════════════════════════
              # Red Hat Validated ModelCar - Full precision for maximum quality
              # - 24B parameters, BF16 precision
              # - State-of-the-art reasoning and function calling
              # - 32k context window
              register_model \
                "Mistral-Small-24B-Instruct-BF16" \
                "Mistral Small 24B Instruct - Red Hat Validated ModelCar. Full Precision BF16 for 4-GPU tensor parallel deployment." \
                '{"mistral":{"metadataType":"MetadataStringValue","string_value":""},"bf16":{"metadataType":"MetadataStringValue","string_value":""},"validated":{"metadataType":"MetadataStringValue","string_value":""},"vllm":{"metadataType":"MetadataStringValue","string_value":""},"modelcar":{"metadataType":"MetadataStringValue","string_value":""},"agentic":{"metadataType":"MetadataStringValue","string_value":""}}' \
                "v1.5-BF16" \
                '{"License":{"metadataType":"MetadataStringValue","string_value":"Apache-2.0"},"Provider":{"metadataType":"MetadataStringValue","string_value":"Mistral AI / Red Hat"},"precision":{"metadataType":"MetadataStringValue","string_value":"bfloat16"},"gpu_requirement":{"metadataType":"MetadataStringValue","string_value":"4x NVIDIA L4 (g6.12xlarge)"},"tensor_parallel":{"metadataType":"MetadataStringValue","string_value":"4"},"context_length":{"metadataType":"MetadataStringValue","string_value":"32768"},"features":{"metadataType":"MetadataStringValue","string_value":"function-calling,multilingual"},"validated_on":{"metadataType":"MetadataStringValue","string_value":"RHOAI 3.0 GA"}}' \
                "oci://registry.redhat.io/rhelai1/modelcar-mistral-small-24b-instruct-2501:1.5" \
                "mistralai/Mistral-Small-24B-Instruct-2501" \
                "vLLM"
              
              # ═══════════════════════════════════════════════════════════════════
              # Model 2: Mistral Small 24B INT4 W4A16 (1-GPU, Red Hat Validated)
              # ═══════════════════════════════════════════════════════════════════
              # Red Hat Validated ModelCar - Neural Magic INT4 quantization
              # - 4-bit weights: ~13.5GB (75% reduction from BF16)
              # - 98.9% accuracy recovery vs full precision
              # - Native vLLM support via compressed-tensors
              register_model \
                "Mistral-Small-24B-Instruct-INT4" \
                "Mistral Small 24B Instruct - INT4 W4A16 quantized by Neural Magic. Red Hat Validated ModelCar for single L4 deployment." \
                '{"mistral":{"metadataType":"MetadataStringValue","string_value":""},"int4":{"metadataType":"MetadataStringValue","string_value":""},"w4a16":{"metadataType":"MetadataStringValue","string_value":""},"validated":{"metadataType":"MetadataStringValue","string_value":""},"vllm":{"metadataType":"MetadataStringValue","string_value":""},"neural-magic":{"metadataType":"MetadataStringValue","string_value":""},"modelcar":{"metadataType":"MetadataStringValue","string_value":""}}' \
                "v1.5-INT4-W4A16" \
                '{"License":{"metadataType":"MetadataStringValue","string_value":"Apache-2.0"},"Provider":{"metadataType":"MetadataStringValue","string_value":"Neural Magic / Red Hat"},"quantization":{"metadataType":"MetadataStringValue","string_value":"int4-w4a16"},"bits":{"metadataType":"MetadataStringValue","string_value":"4"},"accuracy_recovery":{"metadataType":"MetadataStringValue","string_value":"98.9%"},"optimized_for":{"metadataType":"MetadataStringValue","string_value":"nvidia-l4"},"gpu_requirement":{"metadataType":"MetadataStringValue","string_value":"1x NVIDIA L4 (g6.4xlarge)"},"vram_usage":{"metadataType":"MetadataStringValue","string_value":"~13.5GB"},"context_length":{"metadataType":"MetadataStringValue","string_value":"4096"},"cost_savings":{"metadataType":"MetadataStringValue","string_value":"75% vs BF16"},"validated_on":{"metadataType":"MetadataStringValue","string_value":"RHOAI 3.0 GA"}}' \
                "oci://registry.redhat.io/rhelai1/modelcar-mistral-small-24b-instruct-2501-quantized-w4a16:1.5" \
                "neuralmagic/Mistral-Small-24B-Instruct-2501-quantized.w4a16" \
                "vLLM"
              
              echo ""
              echo "════════════════════════════════════════════════════════════════"
              echo "✓ Model Registration Complete"
              echo ""
              echo "  Red Hat Validated ModelCar Pattern:"
              echo "  ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐"
              echo "  │  Model Registry │ →  │  Red Hat OCI    │ →  │     KServe      │"
              echo "  │   (Metadata)    │    │   (ModelCar)    │    │   (Execution)   │"
              echo "  └─────────────────┘    └─────────────────┘    └─────────────────┘"
              echo ""
              echo "  Models Registered (Both Red Hat Validated ModelCars):"
              echo ""
              echo "    1. Mistral-Small-24B-Instruct-BF16"
              echo "       └── 4-GPU (g6.12xlarge) | BF16 | 48GB VRAM | 32k context"
              echo "       └── registry.redhat.io/rhelai1/modelcar-mistral-small-24b-instruct-2501:1.5"
              echo "       └── Features: function-calling, multilingual, agentic"
              echo ""
              echo "    2. Mistral-Small-24B-Instruct-INT4"
              echo "       └── 1-GPU (g6.4xlarge) | INT4 W4A16 | 13.5GB | 4k context"
              echo "       └── registry.redhat.io/rhelai1/modelcar-mistral-small-24b-...-quantized-w4a16:1.5"
              echo "       └── 98.9% accuracy, 75% cost savings"
              echo ""
              echo "  Benefits of Red Hat ModelCars:"
              echo "    • Pre-built, validated OCI images"
              echo "    • No S3/MinIO upload required"
              echo "    • Tested for RHOAI 3.0 compatibility"
              echo "    • Supported by Red Hat"
              echo ""
              echo "  Deploy from GenAI Studio or apply InferenceService manifests"
              echo "════════════════════════════════════════════════════════════════"
