# =============================================================================
# InferenceService: Mistral Small 24B Instruct (4-GPU) with OCI Image Volume
# =============================================================================
# Uses OpenShift 4.20 OCI Image Volume feature to mount the 94GB model
# directly from the registry, bypassing CRI-O extraction issues.
#
# Hardware: g6.12xlarge (4x NVIDIA L4, 96GB total VRAM)
# Use Case: High-throughput production workloads with tensor parallelism
# =============================================================================
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: mistral-small-24b-tp4
  namespace: private-ai
  labels:
    opendatahub.io/dashboard: "true"
    kueue.x-k8s.io/queue-name: default
    networking.kserve.io/visibility: exposed
  annotations:
    argocd.argoproj.io/sync-wave: "10"
    opendatahub.io/hardware-profile-name: nvidia-l4-4gpu
    opendatahub.io/hardware-profile-namespace: redhat-ods-applications
    openshift.io/display-name: "Mistral Small 24B TP4 (4-GPU BF16)"
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  predictor:
    minReplicas: 1
    maxReplicas: 1
    tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
    nodeSelector:
      node.kubernetes.io/instance-type: g6.12xlarge
    containers:
      - name: kserve-container
        # Thin vLLM runtime image (~3GB, not the 94GB model)
        image: registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:ad756c01ec99a99cc7d93401c41b8d92ca96fb1ab7c5262919d818f2be4f3768
        command:
          - python
          - -m
          - vllm.entrypoints.openai.api_server
        args:
          - --port=8080
          - --model=/mnt/models
          - --served-model-name=mistral-small-24b-tp4
          - --tensor-parallel-size=4
          - --dtype=bfloat16
          - --max-model-len=16384
        env:
          - name: HF_HOME
            value: /tmp/hf_home
        ports:
          - containerPort: 8080
            protocol: TCP
        resources:
          requests:
            cpu: "16"
            memory: 64Gi
            nvidia.com/gpu: "4"
          limits:
            cpu: "32"
            memory: 128Gi
            nvidia.com/gpu: "4"
        volumeMounts:
          - mountPath: /dev/shm
            name: shm
          - mountPath: /mnt/models
            name: model-weights
            readOnly: true
    volumes:
      - name: shm
        emptyDir:
          medium: Memory
          sizeLimit: 8Gi
      # OCI Image Volume - OpenShift 4.20 feature
      # Mounts the 94GB model directly from registry without CRI-O extraction
      - name: model-weights
        image:
          reference: registry.redhat.io/rhelai1/mistral-small-24b-instruct-2501:1.5
          pullPolicy: Always  # Force fresh pull to bypass corrupted cache
