# =============================================================================
# InferenceService: Mistral 3 24B Instruct (Full Precision BF16)
# =============================================================================
# Deploys Mistral Small 3 using 4x NVIDIA L4 GPUs with tensor parallelism.
# Configuration aligned with RHOAI 3.0 Dashboard-deployed models.
#
# Hardware: g6.12xlarge (4x NVIDIA L4, 96GB total VRAM)
# Use Case: Production workloads requiring highest accuracy
# =============================================================================
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: mistral-3-bf16
  namespace: private-ai
  labels:
    opendatahub.io/dashboard: "true"
    kueue.x-k8s.io/queue-name: default
    networking.kserve.io/visibility: exposed
  annotations:
    argocd.argoproj.io/sync-wave: "10"
    # Hardware Profile integration
    opendatahub.io/hardware-profile-name: nvidia-l4-4gpu
    opendatahub.io/hardware-profile-namespace: redhat-ods-applications
    # Display name for Dashboard
    openshift.io/display-name: "Mistral 3 24B BF16 (4-GPU)"
    # Deployment mode
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  predictor:
    minReplicas: 1
    maxReplicas: 1
    model:
      modelFormat:
        name: vLLM
      name: ""
      runtime: vllm-runtime
      # S3 storage reference via Data Connection
      storage:
        key: minio-connection
        path: mistral-3-24b-instruct/
      resources:
        requests:
          cpu: "8"
          memory: 64Gi
          nvidia.com/gpu: "4"
        limits:
          cpu: "16"
          memory: 128Gi
          nvidia.com/gpu: "4"
