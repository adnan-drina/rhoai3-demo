# =============================================================================
# InferenceService: Mistral Small 24B Instruct (1-GPU)
# =============================================================================
# Deploys Mistral Small 24B using Red Hat validated OCI image.
# Uses 1x NVIDIA L4 GPU - the model fits in 24GB VRAM.
#
# Hardware: g6.4xlarge (1x NVIDIA L4, 24GB VRAM)
# Use Case: Cost-efficient inference for production workloads
# =============================================================================
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: mistral-small-24b
  namespace: private-ai
  labels:
    opendatahub.io/dashboard: "true"
    kueue.x-k8s.io/queue-name: default
    networking.kserve.io/visibility: exposed
  annotations:
    argocd.argoproj.io/sync-wave: "10"
    # Hardware Profile integration
    opendatahub.io/hardware-profile-name: nvidia-l4-1gpu
    opendatahub.io/hardware-profile-namespace: redhat-ods-applications
    # Display name for Dashboard
    openshift.io/display-name: "Mistral Small 24B (1-GPU)"
    # Deployment mode
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  predictor:
    minReplicas: 1
    maxReplicas: 1
    # nodeSelector and tolerations are injected by Kueue via ResourceFlavor
    # HardwareProfile (nvidia-l4-1gpu) → LocalQueue → ClusterQueue → ResourceFlavor
    model:
      modelFormat:
        name: vLLM
      name: ""
      runtime: vllm-runtime
      # Red Hat validated Mistral model via OCI registry
      storageUri: oci://registry.redhat.io/rhelai1/mistral-small-24b-instruct-2501:1.5
      # vLLM optimization args for single L4 GPU
      args:
        - --quantization=fp8           # FP8 quantization for 50% VRAM reduction
        - --kv-cache-dtype=fp8         # FP8 KV cache for memory efficiency
        - --max-model-len=8192         # Reasonable context for single GPU
      resources:
        requests:
          cpu: "8"
          memory: 32Gi
          nvidia.com/gpu: "1"
          ephemeral-storage: "60Gi"  # Model weights + cache
        limits:
          cpu: "16"
          memory: 60Gi
          nvidia.com/gpu: "1"
          ephemeral-storage: "100Gi"
