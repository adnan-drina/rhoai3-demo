# =============================================================================
# InferenceService: Mistral 3 24B Instruct FP8 (Quantized)
# =============================================================================
# Deploys Mistral Small 3 FP8-quantized using 1x NVIDIA L4 GPU.
# Neural Magic optimized weights for Ada Lovelace FP8 acceleration.
#
# Hardware: g6.4xlarge (1x NVIDIA L4, 24GB VRAM, ~15GB used)
# vLLM Args: --quantization fp8 --kv-cache-dtype fp8
#
# Key Benefits:
# - 50% memory reduction vs BF16
# - Native FP8 hardware acceleration on L4
# - Near-identical accuracy (Neural Magic kernels)
# - 4x cost reduction (1 GPU vs 4 GPUs)
#
# Use Case: Cost-efficient inference for most production workloads
# =============================================================================
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: mistral-3-fp8
  namespace: private-ai
  labels:
    opendatahub.io/dashboard: "true"
    kueue.x-k8s.io/queue-name: private-ai-local-queue
  annotations:
    argocd.argoproj.io/sync-wave: "10"
    openshift.io/display-name: "Mistral 3 24B FP8 (1-GPU, Optimized)"
    serving.kserve.io/deploymentMode: RawDeployment
    serving.kserve.io/autoscalerClass: hpa
    serving.kserve.io/minScale: "1"
    serving.kserve.io/maxScale: "3"
spec:
  predictor:
    model:
      modelFormat:
        name: vllm
      runtime: vllm-runtime
      storageUri: "s3://models/mistral-3-24b-instruct-fp8/"
      args:
        - "--model"
        - "/mnt/models"
        - "--quantization"
        - "fp8"
        - "--kv-cache-dtype"
        - "fp8"
        - "--max-model-len"
        - "8192"
        - "--gpu-memory-utilization"
        - "0.9"
    resources:
      requests:
        cpu: "4"
        memory: "32Gi"
        nvidia.com/gpu: "1"
      limits:
        cpu: "8"
        memory: "64Gi"
        nvidia.com/gpu: "1"
    nodeSelector:
      nvidia.com/gpu.product: NVIDIA-L4
    tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule

