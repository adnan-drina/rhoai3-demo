# =============================================================================
# InferenceService: Mistral-3 INT4 (1-GPU Secondary Load)
# =============================================================================
# The SECONDARY model in the "Resource Handover" demo.
#
# Model: Red Hat Validated ModelCar - Neural Magic INT4 W4A16
# Source: registry.redhat.io/rhelai1/modelcar-mistral-small-24b-instruct-2501-quantized-w4a16:1.5
# Quantization: INT4 weights, FP16 activations
#   - 4-bit weights: ~12GB (75% reduction from BF16)
#   - 98.9% accuracy recovery vs full precision
#
# Hardware: g6.4xlarge (1x NVIDIA L4, 24GB VRAM)
#   - Weights: 12GB
#   - System overhead: ~1.5GB
#   - KV Cache headroom: ~10.5GB (doubled with FP8 cache → ~20GB effective)
#
# Memory Optimization (Red Hat AI Field Engineering):
#   --kv-cache-dtype=fp8     → Doubles KV cache capacity
#   --max-model-len=8192     → Sweet spot for 24B on L4
#   --enable-chunked-prefill → Prevents prefill spikes blocking users
#   --gpu-memory-utilization=0.85 → Driver breathing room
#
# Demo Role: This model consumes 1/5 of the cluster's GPU quota.
# Combined with mistral-3-bf16 (4 GPUs), achieves 100% saturation (5/5).
# =============================================================================
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: mistral-3-int4
  namespace: private-ai
  labels:
    # Kubernetes recommended labels
    app.kubernetes.io/name: mistral-3-int4
    app.kubernetes.io/component: inference
    app.kubernetes.io/part-of: llm-serving
    app.kubernetes.io/managed-by: argocd
    # OpenShift AI Dashboard labels
    opendatahub.io/dashboard: "true"
    opendatahub.io/genai-asset: "true"  # Makes model available in AI Asset Endpoints for Playground
    # Kueue/KServe labels
    kueue.x-k8s.io/queue-name: default
    networking.kserve.io/visibility: exposed
  annotations:
    argocd.argoproj.io/sync-wave: "10"
    opendatahub.io/hardware-profile-name: nvidia-l4-1gpu
    opendatahub.io/hardware-profile-namespace: redhat-ods-applications
    openshift.io/display-name: "Mistral-3 INT4 (1-GPU)"
    serving.kserve.io/deploymentMode: RawDeployment
    # AI Asset / Playground metadata
    opendatahub.io/model-type: generative
    opendatahub.io/genai-use-case: "chat assistant"
    security.opendatahub.io/enable-auth: "false"
spec:
  predictor:
    # SECONDARY LOAD: Always running in baseline state
    minReplicas: 1
    maxReplicas: 1
    # Deployment Strategy: Recreate prevents Kueue deadlock
    # With RollingUpdate, new pods are SchedulingGated by Kueue while old pods
    # hold GPU quota, causing a deadlock. Recreate terminates old pods first.
    deploymentStrategy:
      type: Recreate
    # Tolerations: Required for GPU node taints
    tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
    nodeSelector:
      node.kubernetes.io/instance-type: g6.4xlarge
    model:
      modelFormat:
        name: vLLM
      runtime: vllm-runtime
      # Red Hat Validated ModelCar - OCI Image with INT4 weights (~13.5GB)
      # Fits in CRI-O overlay (< 20GB limit)
      storageUri: oci://registry.redhat.io/rhelai1/modelcar-mistral-small-24b-instruct-2501-quantized-w4a16:1.5
      args:
        - --served-model-name=mistral-3-int4
        # Memory-optimized config for 1x L4 (24GB) - Red Hat AI Field Engineering
        # 8K context = sweet spot for 24B models on L4 hardware
        - --max-model-len=8192
        # FP8 KV cache doubles effective cache capacity (10.5GB → 20GB effective)
        - --kv-cache-dtype=fp8
        # Chunked prefill prevents long prompts from blocking other users
        - --enable-chunked-prefill
        # 85% utilization gives driver breathing room under high load
        - --gpu-memory-utilization=0.85
        - --trust-remote-code
      env:
        # RHOAI 3.0 GA: V1 engine required
        # Driver 570.195.03 provides CUDA 12.x compatibility
        - name: VLLM_USE_V1
          value: "1"
        - name: VLLM_NO_HW_METRICS
          value: "1"
        - name: LD_LIBRARY_PATH
          value: /usr/local/nvidia/lib64
      resources:
        requests:
          cpu: "8"
          memory: 32Gi
          nvidia.com/gpu: "1"
        limits:
          cpu: "16"
          memory: 60Gi
          nvidia.com/gpu: "1"

