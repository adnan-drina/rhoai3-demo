# =============================================================================
# InferenceService: Mistral Small 24B Instruct (1-GPU FP8)
# =============================================================================
# RHOAI 3.0 OFFICIAL APPROACH: S3 Storage + KServe Storage Initializer
#
# The model weights are downloaded from MinIO (S3) at pod startup by the
# storage-initializer init container. This avoids CRI-O extraction issues
# with large OCI images.
#
# Hardware: g6.4xlarge (1x NVIDIA L4, 24GB VRAM)
# Use Case: Cost-efficient inference with FP8 quantization
#
# Prerequisites:
#   1. Model weights uploaded to: s3://models/mistral-small-24b-fp8/
#   2. storage-config secret with minio-connection key in private-ai namespace
# =============================================================================
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: mistral-small-24b
  namespace: private-ai
  labels:
    opendatahub.io/dashboard: "true"
    kueue.x-k8s.io/queue-name: default
    networking.kserve.io/visibility: exposed
  annotations:
    argocd.argoproj.io/sync-wave: "10"
    opendatahub.io/hardware-profile-name: nvidia-l4-1gpu
    opendatahub.io/hardware-profile-namespace: redhat-ods-applications
    openshift.io/display-name: "Mistral Small 24B (1-GPU FP8)"
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  predictor:
    minReplicas: 1
    maxReplicas: 1
    tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
    nodeSelector:
      node.kubernetes.io/instance-type: g6.4xlarge
    model:
      modelFormat:
        name: vLLM
      runtime: vllm-runtime
      # S3 Storage URI - KServe storage-initializer downloads weights at startup
      storageUri: s3://models/mistral-small-24b-fp8/
      storage:
        key: minio-connection
      args:
        # Neural Magic FP8 uses compressed-tensors format - auto-detected by vLLM
        - --max-model-len=8192
        - --kv-cache-dtype=auto
        - --gpu-memory-utilization=0.9
        - --trust-remote-code
      env:
        # RHOAI 3.0 GA stable configuration for 580.x driver compatibility
        - name: VLLM_USE_V1
          value: "0"
        - name: VLLM_ENGINE_SOURCE
          value: "v0"
        - name: VLLM_ATTENTION_BACKEND
          value: "XFORMERS"
        - name: VLLM_NO_HW_METRICS
          value: "1"
        - name: LD_LIBRARY_PATH
          value: /usr/local/nvidia/lib64
      resources:
        requests:
          cpu: "8"
          memory: 32Gi
          nvidia.com/gpu: "1"
        limits:
          cpu: "16"
          memory: 60Gi
          nvidia.com/gpu: "1"

