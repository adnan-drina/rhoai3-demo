# =============================================================================
# InferenceService: Mistral Small 24B Instruct (4-GPU BF16)
# =============================================================================
# RED HAT VALIDATED MODELCAR - Full Precision BF16
#
# Model: registry.redhat.io/rhelai1/modelcar-mistral-small-24b-instruct-2501:1.5
# Precision: BF16 (Full) - 24B parameters, ~48GB VRAM
#   - State-of-the-art conversational and reasoning capabilities
#   - Multilingual: English, French, German, Spanish, Italian, Chinese, etc.
#   - Agent-Centric: Native function calling and JSON outputting
#   - 32k context window
#
# Hardware: g6.12xlarge (4x NVIDIA L4, 96GB total VRAM)
# Use Case: High-throughput production with tensor parallelism
# =============================================================================
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: mistral-small-24b-tp4
  namespace: private-ai
  labels:
    opendatahub.io/dashboard: "true"
    kueue.x-k8s.io/queue-name: default
    networking.kserve.io/visibility: exposed
  annotations:
    argocd.argoproj.io/sync-wave: "10"
    opendatahub.io/hardware-profile-name: nvidia-l4-4gpu
    opendatahub.io/hardware-profile-namespace: redhat-ods-applications
    openshift.io/display-name: "Mistral Small 24B TP4 (4-GPU BF16)"
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  predictor:
    minReplicas: 1
    maxReplicas: 1
    tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
    nodeSelector:
      node.kubernetes.io/instance-type: g6.12xlarge
    model:
      modelFormat:
        name: vLLM
      runtime: vllm-runtime
      # Red Hat Validated ModelCar - OCI Image with BF16 weights
      storageUri: oci://registry.redhat.io/rhelai1/modelcar-mistral-small-24b-instruct-2501:1.5
      args:
        # BF16 Full Precision - optimized for 4x L4 (96GB total VRAM)
        # Tensor Parallel across 4 GPUs for maximum throughput
        - --served-model-name=mistral-small-24b-tp4
        - --tensor-parallel-size=4
        - --dtype=bfloat16
        - --max-model-len=32768
        - --gpu-memory-utilization=0.9
        - --trust-remote-code
        # Agentic features - function calling and tool use
        - --tool-call-parser=mistral
        - --enable-auto-tool-choice
      env:
        # RHOAI 3.0 GA: V1 engine required (assertion enforced by image)
        # Driver 570.195.03 provides CUDA 12.x compatibility
        - name: VLLM_USE_V1
          value: "1"
        - name: VLLM_NO_HW_METRICS
          value: "1"
        - name: LD_LIBRARY_PATH
          value: /usr/local/nvidia/lib64
      resources:
        requests:
          cpu: "16"
          memory: 64Gi
          nvidia.com/gpu: "4"
        limits:
          cpu: "32"
          memory: 128Gi
          nvidia.com/gpu: "4"
