# =============================================================================
# InferenceService: Devstral-2 (4-GPU Queued Asset)
# =============================================================================
# The "WAITING" model in the "Resource Handover" demo.
#
# Model: Mistral Small 24B Instruct (placeholder for Devstral-2)
# Storage: s3://models/mistral-small-24b/ (shares weights with BF16 for demo)
# Hardware: g6.12xlarge (4x NVIDIA L4, 96GB total VRAM)
#
# Demo Role: This model is INTENTIONALLY set to minReplicas: 0
# When activated via the GPU Switchboard, it will be QUEUED by Kueue
# because the cluster is already at 100% GPU capacity (5/5).
#
# The Demo Flow:
#   1. Start: mistral-3-bf16 (4) + mistral-3-int4 (1) = 5/5 GPUs used
#   2. Action: Set devstral-2 minReplicas → 1
#   3. Result: Devstral-2 stays PENDING (4 + 1 + 4 > 5)
#   4. Resolution: Set mistral-3-bf16 minReplicas → 0
#   5. Climax: Devstral-2 INSTANTLY starts as 4 GPUs become available
#
# NOTE: Uses same S3 weights as BF16 for demo simplicity.
# In production, this would point to dedicated Devstral-2 weights.
# =============================================================================
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: devstral-2
  namespace: private-ai
  labels:
    opendatahub.io/dashboard: "true"
    kueue.x-k8s.io/queue-name: default
    networking.kserve.io/visibility: exposed
  annotations:
    argocd.argoproj.io/sync-wave: "10"
    opendatahub.io/hardware-profile-name: nvidia-l4-4gpu
    opendatahub.io/hardware-profile-namespace: redhat-ods-applications
    openshift.io/display-name: "Devstral-2 (4-GPU Agentic)"
    serving.kserve.io/deploymentMode: RawDeployment
spec:
  predictor:
    # QUEUED ASSET: Starts at 0 replicas
    # Toggle to 1 via GPU Switchboard to demonstrate queueing
    minReplicas: 0
    maxReplicas: 1
    # Tolerations: Required for GPU node taints
    tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
    nodeSelector:
      node.kubernetes.io/instance-type: g6.12xlarge
    model:
      modelFormat:
        name: vLLM
      runtime: vllm-runtime
      # S3 Storage - shares BF16 weights for demo (Devstral-2 is same base model)
      storageUri: s3://models/mistral-small-24b/
      storage:
        key: minio-connection
      args:
        - --served-model-name=devstral-2
        - --tensor-parallel-size=4
        - --dtype=bfloat16
        - --max-model-len=32768
        - --gpu-memory-utilization=0.9
        - --trust-remote-code
        # Agentic features - optimized for tool use
        - --tool-call-parser=mistral
        - --enable-auto-tool-choice
      env:
        # RHOAI 3.0 GA: V1 engine required
        - name: VLLM_USE_V1
          value: "1"
        - name: VLLM_NO_HW_METRICS
          value: "1"
        - name: LD_LIBRARY_PATH
          value: /usr/local/nvidia/lib64
      resources:
        requests:
          cpu: "16"
          memory: 64Gi
          nvidia.com/gpu: "4"
        limits:
          cpu: "32"
          memory: 128Gi
          nvidia.com/gpu: "4"

