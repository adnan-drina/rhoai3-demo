# =============================================================================
# GPU-Switchboard Notebook ConfigMap
# =============================================================================
# Contains the pre-loaded Jupyter notebook for the GPU-as-a-Service demo.
# This ConfigMap is mounted into the workbench at /opt/app-root/src/notebooks/
# =============================================================================
apiVersion: v1
kind: ConfigMap
metadata:
  name: gpu-switchboard-notebook
  namespace: private-ai
  labels:
    app: gpu-switchboard
  annotations:
    argocd.argoproj.io/sync-wave: "5"
data:
  GPU-Switchboard.ipynb: |
    {
      "cells": [
        {
          "cell_type": "markdown",
          "metadata": {},
          "source": [
            "# üéõÔ∏è GPU Switchboard: Enterprise Model Portfolio\n",
            "\n",
            "This notebook demonstrates **RHOAI 3.0's GPU-as-a-Service** capabilities using Kueue for dynamic resource allocation.\n",
            "\n",
            "## The Demo Scenario\n",
            "\n",
            "Our cluster has **5 GPUs** available:\n",
            "- 1x `g6.12xlarge` ‚Üí 4 GPUs\n",
            "- 1x `g6.4xlarge` ‚Üí 1 GPU\n",
            "\n",
            "**Enterprise Model Portfolio (5 Red Hat Validated Models):**\n",
            "\n",
            "| Model | GPUs | Status | Use Case |\n",
            "|-------|------|--------|----------|\n",
            "| `mistral-3-bf16` | 4 | ‚úÖ Active | Primary Production |\n",
            "| `mistral-3-int4` | 1 | ‚úÖ Active | Cost-efficient (75% savings) |\n",
            "| `devstral-2` | 4 | ‚è∏Ô∏è Queued | Agentic Tool-calling |\n",
            "| `gpt-oss-20b` | 4 | ‚è∏Ô∏è Queued | High-reasoning (Oct 2025) |\n",
            "| `granite-8b-agent` | 1 | ‚è∏Ô∏è Queued | RAG/Tool-call (May 2025) |\n",
            "\n",
            "**Total Potential:** 14 GPUs | **Quota Limit:** 5 GPUs\n",
            "\n",
            "**The Challenge:** How does Kueue manage competing requests for limited GPUs?\n",
            "\n",
            "**The Answer:** Intelligent queuing + dynamic resource handover!"
          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {},
          "source": [
            "## Setup: Import Libraries and Check Cluster State"
          ]
        },
        {
          "cell_type": "code",
          "execution_count": null,
          "metadata": {},
          "outputs": [],
          "source": [
            "import subprocess\n",
            "import json\n",
            "from IPython.display import display, HTML, clear_output\n",
            "import ipywidgets as widgets\n",
            "import time\n",
            "\n",
            "NAMESPACE = \"private-ai\"\n",
            "\n",
            "# Enterprise Model Portfolio - 5 Red Hat Validated Models\n",
            "MODELS = {\n",
            "    \"mistral-3-bf16\": {\n",
            "        \"gpus\": 4, \n",
            "        \"color\": \"#2196F3\",\n",
            "        \"desc\": \"BF16 Full Precision (Primary Load)\",\n",
            "        \"node\": \"g6.12xlarge\",\n",
            "        \"provider\": \"Mistral AI\"\n",
            "    },\n",
            "    \"mistral-3-int4\": {\n",
            "        \"gpus\": 1, \n",
            "        \"color\": \"#4CAF50\",\n",
            "        \"desc\": \"INT4 Quantized (75% cost savings)\",\n",
            "        \"node\": \"g6.4xlarge\",\n",
            "        \"provider\": \"Neural Magic / Red Hat\"\n",
            "    },\n",
            "    \"devstral-2\": {\n",
            "        \"gpus\": 4, \n",
            "        \"color\": \"#FF9800\",\n",
            "        \"desc\": \"Agentic Model (Tool-calling)\",\n",
            "        \"node\": \"g6.12xlarge\",\n",
            "        \"provider\": \"Mistral AI\"\n",
            "    },\n",
            "    \"gpt-oss-20b\": {\n",
            "        \"gpus\": 4, \n",
            "        \"color\": \"#9C27B0\",\n",
            "        \"desc\": \"High-Reasoning (Oct 2025 Collection)\",\n",
            "        \"node\": \"g6.12xlarge\",\n",
            "        \"provider\": \"RedHatAI\"\n",
            "    },\n",
            "    \"granite-8b-agent\": {\n",
            "        \"gpus\": 1, \n",
            "        \"color\": \"#E91E63\",\n",
            "        \"desc\": \"Small but Mighty - RAG/Tool-call\",\n",
            "        \"node\": \"g6.4xlarge\",\n",
            "        \"provider\": \"IBM / Red Hat\"\n",
            "    }\n",
            "}\n",
            "\n",
            "def run_oc(args):\n",
            "    result = subprocess.run([\"oc\"] + args, capture_output=True, text=True)\n",
            "    return result.stdout.strip(), result.returncode\n",
            "\n",
            "def get_model_status(name):\n",
            "    out, _ = run_oc([\"get\", \"inferenceservice\", name, \"-n\", NAMESPACE, \"-o\", \"json\"])\n",
            "    if out:\n",
            "        try:\n",
            "            data = json.loads(out)\n",
            "            min_replicas = data.get(\"spec\", {}).get(\"predictor\", {}).get(\"minReplicas\", 0)\n",
            "            conditions = data.get(\"status\", {}).get(\"conditions\", [])\n",
            "            ready = \"Unknown\"\n",
            "            for cond in conditions:\n",
            "                if cond.get(\"type\") == \"Ready\":\n",
            "                    ready = cond.get(\"status\", \"Unknown\")\n",
            "                    break\n",
            "            return min_replicas, ready\n",
            "        except json.JSONDecodeError:\n",
            "            return 0, \"Error\"\n",
            "    return 0, \"Unknown\"\n",
            "\n",
            "def set_model_replicas(name, replicas):\n",
            "    patch = json.dumps({\"spec\": {\"predictor\": {\"minReplicas\": replicas}}})\n",
            "    out, code = run_oc([\"patch\", \"inferenceservice\", name, \"-n\", NAMESPACE, \n",
            "                        \"--type=merge\", \"-p\", patch])\n",
            "    return code == 0\n",
            "\n",
            "print(\"‚úÖ Setup complete. Connected to namespace:\", NAMESPACE)\n",
            "print(f\"üìä Model Portfolio: {len(MODELS)} models | Total Potential: {sum(m['gpus'] for m in MODELS.values())} GPUs | Quota: 5 GPUs\")"
          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {},
          "source": [
            "## Current Cluster Status\n",
            "\n",
            "Let's check the current GPU allocation and model states:"
          ]
        },
        {
          "cell_type": "code",
          "execution_count": null,
          "metadata": {},
          "outputs": [],
          "source": [
            "def show_cluster_status():\n",
            "    total_gpus = 5\n",
            "    used_gpus = 0\n",
            "    \n",
            "    html = \"<h3>üìä Enterprise Model Portfolio Status</h3>\"\n",
            "    html += \"<table style='width:100%; border-collapse: collapse; font-family: monospace;'>\"\n",
            "    html += \"<tr style='background:#1a1a2e; color:white;'>\"\n",
            "    html += \"<th style='padding:10px; text-align:left;'>Model</th>\"\n",
            "    html += \"<th style='padding:10px; text-align:center;'>GPUs</th>\"\n",
            "    html += \"<th style='padding:10px; text-align:center;'>Node</th>\"\n",
            "    html += \"<th style='padding:10px; text-align:center;'>State</th>\"\n",
            "    html += \"<th style='padding:10px; text-align:center;'>Ready</th>\"\n",
            "    html += \"</tr>\"\n",
            "    \n",
            "    for name, config in MODELS.items():\n",
            "        min_rep, ready = get_model_status(name)\n",
            "        state = \"üü¢ ON\" if min_rep > 0 else \"‚ö´ OFF\"\n",
            "        ready_icon = \"‚úÖ\" if ready == \"True\" else (\"‚è≥\" if min_rep > 0 else \"‚Äî\")\n",
            "        \n",
            "        if min_rep > 0:\n",
            "            used_gpus += config[\"gpus\"]\n",
            "        \n",
            "        bg_color = \"#e3f2fd\" if min_rep > 0 else \"#fafafa\"\n",
            "        \n",
            "        html += f\"<tr style='border-bottom:1px solid #ddd; background:{bg_color};'>\"\n",
            "        html += f\"<td style='padding:10px;'>\"\n",
            "        html += f\"<b style='color:{config['color']}'>{name}</b><br>\"\n",
            "        html += f\"<small style='color:#666;'>{config['desc']}</small><br>\"\n",
            "        html += f\"<small style='color:#999;'>Provider: {config['provider']}</small>\"\n",
            "        html += f\"</td>\"\n",
            "        html += f\"<td style='padding:10px; text-align:center; font-weight:bold;'>{config['gpus']}</td>\"\n",
            "        html += f\"<td style='padding:10px; text-align:center; font-size:0.85em;'>{config['node']}</td>\"\n",
            "        html += f\"<td style='padding:10px; text-align:center;'>{state}</td>\"\n",
            "        html += f\"<td style='padding:10px; text-align:center;'>{ready_icon}</td>\"\n",
            "        html += \"</tr>\"\n",
            "    \n",
            "    html += \"</table>\"\n",
            "    \n",
            "    pct = (used_gpus / total_gpus) * 100\n",
            "    if pct < 60:\n",
            "        bar_color = \"#4CAF50\"\n",
            "    elif pct < 80:\n",
            "        bar_color = \"#FF9800\"\n",
            "    elif pct < 100:\n",
            "        bar_color = \"#2196F3\"\n",
            "    else:\n",
            "        bar_color = \"#f44336\"\n",
            "    \n",
            "    html += f\"<h3>üîã GPU Quota: {used_gpus}/{total_gpus}</h3>\"\n",
            "    html += f\"<div style='background:#e0e0e0; border-radius:10px; height:30px; width:100%; overflow:hidden;'>\"\n",
            "    html += f\"<div style='background:{bar_color}; width:{pct}%; height:100%; \"\n",
            "    html += f\"text-align:center; line-height:30px; color:white; font-weight:bold; \"\n",
            "    html += f\"transition: width 0.5s ease;'>{pct:.0f}%</div></div>\"\n",
            "    \n",
            "    if pct >= 100:\n",
            "        html += \"<p style='color:#f44336;'><b>‚ö†Ô∏è Quota Saturated!</b> New requests will be queued by Kueue.</p>\"\n",
            "    \n",
            "    display(HTML(html))\n",
            "\n",
            "show_cluster_status()"
          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {},
          "source": [
            "## üéõÔ∏è GPU Switchboard\n",
            "\n",
            "Use these toggles to control which models are running. Watch how Kueue manages the GPU quota!"
          ]
        },
        {
          "cell_type": "code",
          "execution_count": null,
          "metadata": {},
          "outputs": [],
          "source": [
            "output = widgets.Output()\n",
            "\n",
            "def create_toggle(name, config):\n",
            "    min_rep, _ = get_model_status(name)\n",
            "    \n",
            "    toggle = widgets.ToggleButton(\n",
            "        value=(min_rep > 0),\n",
            "        description=f\"{name} ({config['gpus']} GPU)\",\n",
            "        button_style='success' if min_rep > 0 else '',\n",
            "        layout=widgets.Layout(width='320px', height='55px'),\n",
            "        style={'font_weight': 'bold'}\n",
            "    )\n",
            "    \n",
            "    def on_toggle(change):\n",
            "        with output:\n",
            "            clear_output(wait=True)\n",
            "            new_replicas = 1 if change['new'] else 0\n",
            "            action = \"üöÄ Enabling\" if new_replicas else \"üõë Disabling\"\n",
            "            print(f\"{action} {name}...\")\n",
            "            \n",
            "            if set_model_replicas(name, new_replicas):\n",
            "                toggle.button_style = 'success' if new_replicas else ''\n",
            "                print(f\"‚úÖ {name} set to minReplicas={new_replicas}\")\n",
            "                \n",
            "                if new_replicas > 0:\n",
            "                    time.sleep(3)\n",
            "                    pods_out, _ = run_oc([\"get\", \"pods\", \"-n\", NAMESPACE, \n",
            "                                         \"-l\", f\"serving.kserve.io/inferenceservice={name}\",\n",
            "                                         \"--no-headers\"])\n",
            "                    if \"SchedulingGated\" in pods_out or \"Pending\" in pods_out:\n",
            "                        print(f\"\\n‚ö†Ô∏è  {name} is PENDING in Kueue queue!\")\n",
            "                        print(\"   ‚Üí Not enough GPUs available\")\n",
            "                        print(\"   ‚Üí Disable another model to free resources\")\n",
            "                    elif \"Running\" in pods_out:\n",
            "                        print(f\"\\n‚úÖ {name} is now RUNNING!\")\n",
            "                    else:\n",
            "                        print(f\"\\n‚è≥ {name} is starting up...\")\n",
            "            else:\n",
            "                print(f\"‚ùå Failed to update {name}\")\n",
            "            \n",
            "            print(\"\\n\" + \"=\"*60)\n",
            "            show_cluster_status()\n",
            "    \n",
            "    toggle.observe(on_toggle, names='value')\n",
            "    return toggle\n",
            "\n",
            "toggles_4gpu = []\n",
            "toggles_1gpu = []\n",
            "\n",
            "for name, config in MODELS.items():\n",
            "    toggle = create_toggle(name, config)\n",
            "    if config['gpus'] == 4:\n",
            "        toggles_4gpu.append(toggle)\n",
            "    else:\n",
            "        toggles_1gpu.append(toggle)\n",
            "\n",
            "header = widgets.HTML(\"<h2>üéõÔ∏è GPU Switchboard</h2><p>Toggle models ON/OFF to manage GPU allocation</p>\")\n",
            "\n",
            "section_4gpu = widgets.HTML(\"<h4 style='margin-top:15px;'>üñ•Ô∏è 4-GPU Models (g6.12xlarge)</h4>\")\n",
            "box_4gpu = widgets.VBox(toggles_4gpu, layout=widgets.Layout(padding='5px'))\n",
            "\n",
            "section_1gpu = widgets.HTML(\"<h4 style='margin-top:15px;'>üíö 1-GPU Models (g6.4xlarge)</h4>\")\n",
            "box_1gpu = widgets.VBox(toggles_1gpu, layout=widgets.Layout(padding='5px'))\n",
            "\n",
            "display(header)\n",
            "display(section_4gpu)\n",
            "display(box_4gpu)\n",
            "display(section_1gpu)\n",
            "display(box_1gpu)\n",
            "display(widgets.HTML(\"<hr><h3>üìã Operation Log</h3>\"))\n",
            "display(output)"
          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {},
          "source": [
            "## üìà Kueue Workload Status\n",
            "\n",
            "Monitor the Kueue queue to see pending workloads:"
          ]
        },
        {
          "cell_type": "code",
          "execution_count": null,
          "metadata": {},
          "outputs": [],
          "source": [
            "def show_kueue_status():\n",
            "    print(\"üîç Kueue Workloads in private-ai namespace:\\n\")\n",
            "    out, _ = run_oc([\"get\", \"workload\", \"-n\", NAMESPACE, \"-o\", \"wide\"])\n",
            "    print(out if out else \"No workloads found\")\n",
            "    \n",
            "    print(\"\\n\" + \"=\"*70)\n",
            "    print(\"\\nüè¢ ClusterQueue Status:\\n\")\n",
            "    out, _ = run_oc([\"get\", \"clusterqueue\", \"rhoai-main-queue\", \"-o\", \"wide\"])\n",
            "    print(out if out else \"ClusterQueue not found\")\n",
            "\n",
            "show_kueue_status()"
          ]
        },
        {
          "cell_type": "markdown",
          "metadata": {},
          "source": [
            "## üé¨ Demo Scripts\n",
            "\n",
            "### Demo 1: Resource Handover\n",
            "\n",
            "**Story:** \"We need to switch from our general-purpose Mistral to the specialized Devstral agent.\"\n",
            "\n",
            "1. **Baseline:** Verify `mistral-3-bf16` (4) + `mistral-3-int4` (1) = 5/5 GPUs\n",
            "2. **Toggle ON:** Enable `devstral-2` ‚Üí Watch it go to **PENDING**\n",
            "3. **Toggle OFF:** Disable `mistral-3-bf16` ‚Üí Watch `devstral-2` **INSTANTLY START**\n",
            "\n",
            "---\n",
            "\n",
            "### Demo 2: Efficiency Story\n",
            "\n",
            "**Story:** \"We can trade one expensive 4-GPU model for multiple specialized 1-GPU models.\"\n",
            "\n",
            "1. **Before:** `mistral-3-bf16` (4) + `mistral-3-int4` (1) = 5 GPUs\n",
            "2. **Disable:** Turn off `mistral-3-bf16`\n",
            "3. **Enable:** Turn on `granite-8b-agent` (1 GPU) ‚Üí **STARTS INSTANTLY**\n",
            "4. **Result:** 2 specialized models + room for 3 more 1-GPU models!\n",
            "\n",
            "**Message:** \"With the same 5 GPUs, we now serve 4x more specialized workloads.\"\n",
            "\n",
            "---\n",
            "\n",
            "### Demo 3: Priority Queue\n",
            "\n",
            "**Story:** \"Kueue ensures fair access - no one can hoard all GPUs.\"\n",
            "\n",
            "1. **Enable all 5 models** ‚Üí Watch 3 go to **PENDING**\n",
            "2. **Show Kueue status** ‚Üí All 5 are tracked, only 5 GPUs admitted\n",
            "3. **Disable one 4-GPU model** ‚Üí Watch a pending model **AUTO-START**\n",
            "\n",
            "---\n",
            "\n",
            "### The Key Message\n",
            "> \"This is GPU-as-a-Service. The combination of **Model Registry** (governance) and **Kueue** (resource arbitration) enables a 'Dynamic Model Portfolio' strategy. Organizations can provide access to a wide range of Red Hat Validated models while strictly controlling AWS GPU consumption.\""
          ]
        },
        {
          "cell_type": "code",
          "execution_count": null,
          "metadata": {},
          "outputs": [],
          "source": [
            "# Quick refresh of all states\n",
            "print(\"üîÑ Refreshing status...\\n\")\n",
            "show_cluster_status()\n",
            "print(\"\\n\")\n",
            "show_kueue_status()"
          ]
        }
      ],
      "metadata": {
        "kernelspec": {
          "display_name": "Python 3",
          "language": "python",
          "name": "python3"
        },
        "language_info": {
          "name": "python",
          "version": "3.11"
        }
      },
      "nbformat": 4,
      "nbformat_minor": 4
    }

