{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ® GPU-as-a-Service Orchestrator\n",
        "\n",
        "**Platform Engineering Standard Operating Procedure (SOP)**\n",
        "\n",
        "This notebook demonstrates **RHOAI 3.0 GPU quota management** using native OpenShift `oc` commands that any Platform Engineer can use.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“Š Architecture Overview\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    Kueue Quota Management                       â”‚\n",
        "â”‚                                                                 â”‚\n",
        "â”‚   ClusterQueue: rhoai-main-queue                                â”‚\n",
        "â”‚   â”œâ”€â”€ nominalQuota: 5 GPUs                                      â”‚\n",
        "â”‚   â””â”€â”€ ResourceFlavors: nvidia-l4-1gpu, nvidia-l4-4gpu           â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“º Setup: Open Monitoring Dashboard\n",
        "\n",
        "**Before running the demo**, open the RHOAI Dashboard in another tab:\n",
        "\n",
        "1. Navigate to **RHOAI Dashboard** â†’ **Distributed Workloads**\n",
        "2. Select project: **private-ai**\n",
        "3. Watch the **Workloads** table for status changes\n",
        "\n",
        "This provides real-time visibility into Kueue admission decisions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ” Step 0: Check Current State\n",
        "\n",
        "Let's verify the baseline: **5/5 GPUs used** (BF16 + INT4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View all InferenceServices and their replica counts\n",
        "! oc get inferenceservice -n private-ai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check Kueue workloads - shows admitted vs pending\n",
        "! oc get workloads -n private-ai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check running pods (should see bf16 and int4)\n",
        "! oc get pods -n private-ai -l serving.kserve.io/inferenceservice\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸš€ Scenario 1: Resource Conflict\n",
        "\n",
        "**Story:** \"A data scientist requests Devstral-2 for agentic workflows.\"\n",
        "\n",
        "We'll attempt to start Devstral-2 (4 GPUs), but the cluster is already at capacity:\n",
        "- Current: BF16 (4) + INT4 (1) = **5/5 GPUs**\n",
        "- Requested: Devstral-2 (4) = **+4 GPUs**\n",
        "- Result: **Over quota** â†’ Kueue holds in PENDING state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale up Devstral-2 (requests 4 GPUs)\n",
        "! oc patch inferenceservice devstral-2 -n private-ai --type=merge -p '{\"spec\":{\"predictor\":{\"minReplicas\":1}}}'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check workload status - Devstral-2 should be PENDING\n",
        "! oc get workloads -n private-ai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ’¡ Observation\n",
        "\n",
        "In the **Distributed Workloads** dashboard, you should see:\n",
        "- `devstral-2` workload with status: **Pending**\n",
        "- Message: \"Insufficient quota\" or \"Waiting for resources\"\n",
        "\n",
        "**This is Kueue working correctly!** The request is queued, not rejected.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ”„ Scenario 2: Dynamic Handover\n",
        "\n",
        "**Story:** \"Platform team hibernates the BF16 model to free GPUs.\"\n",
        "\n",
        "We'll scale down BF16 to release 4 GPUs. Kueue will **automatically** admit Devstral-2.\n",
        "\n",
        "**Watch the dashboard!** This is the \"wow moment\" of the demo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale down BF16 to release 4 GPUs\n",
        "! oc patch inferenceservice mistral-3-bf16 -n private-ai --type=merge -p '{\"spec\":{\"predictor\":{\"minReplicas\":0}}}'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check the transition - BF16 terminates, Devstral-2 starts\n",
        "! oc get pods -n private-ai -l serving.kserve.io/inferenceservice\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify final state\n",
        "! oc get inferenceservice -n private-ai\n",
        "! echo \"\"\n",
        "! oc get workloads -n private-ai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ’¡ Key Observation\n",
        "\n",
        "**Devstral-2 started INSTANTLY** when BF16 was hibernated.\n",
        "\n",
        "This is **GPU-as-a-Service**: Resources flow to where they're needed, governed by quota.\n",
        "\n",
        "Current State:\n",
        "- Devstral-2 (4) + INT4 (1) = **5/5 GPUs**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ“‰ Scenario 3: Efficiency Portfolio\n",
        "\n",
        "**Story:** \"Trade one 4-GPU model for multiple specialized 1-GPU models.\"\n",
        "\n",
        "Instead of one big model, we'll run:\n",
        "- `granite-8b-agent` (1 GPU) - Agentic tool-calling\n",
        "- `gpt-oss-20b` (1 GPU) - High-reasoning *(if weights available)*\n",
        "- `mistral-3-int4` (1 GPU) - Cost-efficient inference\n",
        "\n",
        "**Same 5 GPU quota, 3x more model diversity!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Hibernate Devstral-2 to free 4 GPUs\n",
        "! oc patch inferenceservice devstral-2 -n private-ai --type=merge -p '{\"spec\":{\"predictor\":{\"minReplicas\":0}}}'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Start specialized 1-GPU models\n",
        "! oc patch inferenceservice granite-8b-agent -n private-ai --type=merge -p '{\"spec\":{\"predictor\":{\"minReplicas\":1}}}'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check the new portfolio\n",
        "! oc get inferenceservice -n private-ai\n",
        "! echo \"\"\n",
        "! oc get workloads -n private-ai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ğŸ’¡ The Efficiency Message\n",
        "\n",
        "**Before:** 1 big model (Devstral-2, 4 GPUs)\n",
        "**After:** 2 specialized models (INT4 + Granite, 2 GPUs)\n",
        "\n",
        "You've traded raw power for **specialization and efficiency**, with room to spare!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ”„ Reset: Return to Baseline State\n",
        "\n",
        "Return to the original \"saturated\" state for the next demo run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reset all models to baseline\n",
        "! oc patch inferenceservice mistral-3-bf16 -n private-ai --type=merge -p '{\"spec\":{\"predictor\":{\"minReplicas\":1}}}'\n",
        "! oc patch inferenceservice mistral-3-int4 -n private-ai --type=merge -p '{\"spec\":{\"predictor\":{\"minReplicas\":1}}}'\n",
        "! oc patch inferenceservice devstral-2 -n private-ai --type=merge -p '{\"spec\":{\"predictor\":{\"minReplicas\":0}}}'\n",
        "! oc patch inferenceservice gpt-oss-20b -n private-ai --type=merge -p '{\"spec\":{\"predictor\":{\"minReplicas\":0}}}'\n",
        "! oc patch inferenceservice granite-8b-agent -n private-ai --type=merge -p '{\"spec\":{\"predictor\":{\"minReplicas\":0}}}'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify baseline restored\n",
        "! oc get inferenceservice -n private-ai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ“– Technical Reference\n",
        "\n",
        "### Why `oc patch` (not `oc scale`) for KServe\n",
        "\n",
        "KServe InferenceService does **not** implement the Kubernetes scale subresource in RHOAI 3.0.\n",
        "Use `oc patch` to modify `spec.predictor.minReplicas`:\n",
        "\n",
        "```bash\n",
        "# Scale up (minReplicas=1 starts the model)\n",
        "oc patch inferenceservice <name> -n private-ai --type=merge \\\n",
        "  -p '{\"spec\":{\"predictor\":{\"minReplicas\":1}}}'\n",
        "\n",
        "# Scale down (minReplicas=0 hibernates the model)\n",
        "oc patch inferenceservice <name> -n private-ai --type=merge \\\n",
        "  -p '{\"spec\":{\"predictor\":{\"minReplicas\":0}}}'\n",
        "```\n",
        "\n",
        "### How Kueue Intercepts Scale Events\n",
        "\n",
        "Because the `private-ai` namespace has `kueue.openshift.io/managed=true`:\n",
        "\n",
        "1. Patch event modifies `minReplicas` â†’ triggers pod creation request\n",
        "2. Kueue intercepts via admission webhook\n",
        "3. Checks against `ClusterQueue` quota (5 GPUs)\n",
        "4. **If under quota:** Workload admitted immediately\n",
        "5. **If over quota:** Workload held in PENDING until resources free\n",
        "\n",
        "### Documentation Links\n",
        "\n",
        "- [RHOAI 3.0: Deploying Models](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0/html-single/deploying_models/index)\n",
        "- [RHOAI 3.0: Managing Distributed Workloads](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0/html-single/working_with_distributed_workloads/index)\n",
        "- [Kueue Documentation](https://kueue.sigs.k8s.io/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ¤ Demo Script (Presenter Notes)\n",
        "\n",
        "### Opening (30 seconds)\n",
        "> \"In enterprise environments, GPU resources are expensive and shared. Teams can't hoard GPUs indefinitely. Let me show you how RHOAI 3.0 manages this with **GPU-as-a-Service**.\"\n",
        "\n",
        "### Scenario 1: The Conflict (1 minute)\n",
        "> \"We have 5 GPUs and a 5 GPU quota. Right now, Mistral BF16 uses 4, and INT4 uses 1. That's 100% utilization.\"\n",
        "> \n",
        "> \"A data scientist requests Devstral-2 for agentic workflows. Let's see what happens...\"\n",
        "> \n",
        "> *Run scale command*\n",
        "> \n",
        "> \"Notice it's PENDINGâ€”not rejected. Kueue is holding it in queue until resources are available.\"\n",
        "\n",
        "### Scenario 2: The Handover (1 minute)\n",
        "> \"As a Platform Engineer, I can hibernate the BF16 model to free those 4 GPUs.\"\n",
        "> \n",
        "> *Run scale down command*\n",
        "> \n",
        "> \"Watch the dashboardâ€”Devstral-2 starts INSTANTLY. This is the magic of Kueue.\"\n",
        "\n",
        "### Closing (30 seconds)\n",
        "> \"This is GPU-as-a-Service. Model Registry provides governance, Kueue provides resource arbitration. Organizations access a wide range of Red Hat Validated models while controlling AWS GPU costs.\"\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
