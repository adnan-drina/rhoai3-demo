{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ® GPU-as-a-Service Orchestrator\n",
        "\n",
        "**Platform Engineering Standard Operating Procedure (SOP)**\n",
        "\n",
        "This notebook demonstrates **RHOAI 3.0 GPU quota management** using native OpenShift `oc` commands that any Platform Engineer can use.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“Š Architecture Overview\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    Kueue Quota Management                       â”‚\n",
        "â”‚                                                                 â”‚\n",
        "â”‚   ClusterQueue: rhoai-main-queue                                â”‚\n",
        "â”‚   â”œâ”€â”€ nominalQuota: 5 GPUs                                      â”‚\n",
        "â”‚   â””â”€â”€ ResourceFlavors: nvidia-l4-1gpu, nvidia-l4-4gpu           â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“º Setup: Open Monitoring Dashboard\n",
        "\n",
        "**Before running the demo**, open the RHOAI Dashboard in another tab:\n",
        "\n",
        "1. Navigate to **RHOAI Dashboard** â†’ **Distributed Workloads**\n",
        "2. Select project: **private-ai**\n",
        "3. Watch the **Workloads** table for status changes\n",
        "\n",
        "This provides real-time visibility into Kueue admission decisions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ” Step 0: Check Current State\n",
        "\n",
        "Let's verify the baseline: **5/5 GPUs used** (BF16 + INT4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View all InferenceServices and their replica counts\n",
        "! oc get inferenceservice -n private-ai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check Kueue workloads - shows admitted vs pending\n",
        "! oc get workloads -n private-ai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check deployments\n",
        "! oc get deployments -n private-ai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check running pods\n",
        "! oc get pods -n private-ai -l serving.kserve.io/inferenceservice\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale up Deployment - IMMEDIATE\n",
        "! oc scale deployment {name}-predictor -n private-ai --replicas=1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale down Deployment - IMMEDIATE\n",
        "! oc scale deployment {name}-predictor -n private-ai --replicas=0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ”„ Reset: Return to Baseline State\n",
        "\n",
        "Return to the original \"saturated\" state for the next demo run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reset all models to baseline - IMMEDIATE via Deployments\n",
        "! oc scale deployment mistral-3-bf16-predictor -n private-ai --replicas=1\n",
        "! oc scale deployment mistral-3-int4-predictor -n private-ai --replicas=1\n",
        "! oc scale deployment devstral-2-predictor -n private-ai --replicas=0\n",
        "! oc scale deployment gpt-oss-20b-predictor -n private-ai --replicas=0\n",
        "! oc scale deployment granite-8b-agent-predictor -n private-ai --replicas=0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify baseline restored\n",
        "! oc get inferenceservice -n private-ai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ“– Technical Reference\n",
        "\n",
        "### Why `oc scale deployment` (not `oc scale inferenceservice`)\n",
        "\n",
        "RHOAI 3.0 uses **RawDeployment mode** by default (not Knative Serving). This means:\n",
        "- No `ksvc` (Knative Services) are created\n",
        "- Each InferenceService creates a Deployment named `{name}-predictor`\n",
        "- `oc scale inferenceservice` doesn't work (scale subresource not implemented)\n",
        "\n",
        "**The elegant solution:** Scale the underlying Deployment directly!\n",
        "\n",
        "```bash\n",
        "# Scale DOWN (immediate - pod terminates instantly)\n",
        "oc scale deployment {name}-predictor -n private-ai --replicas=0\n",
        "\n",
        "# Scale UP (Kueue manages admission via SchedulingGates)\n",
        "oc scale deployment {name}-predictor -n private-ai --replicas=1\n",
        "```\n",
        "\n",
        "### Deployment Naming Convention\n",
        "\n",
        "| InferenceService | Deployment |\n",
        "|-----------------|------------|\n",
        "| `mistral-3-bf16` | `mistral-3-bf16-predictor` |\n",
        "| `mistral-3-int4` | `mistral-3-int4-predictor` |\n",
        "| `devstral-2` | `devstral-2-predictor` |\n",
        "| `granite-8b-agent` | `granite-8b-agent-predictor` |\n",
        "| `gpt-oss-20b` | `gpt-oss-20b-predictor` |\n",
        "\n",
        "### How Kueue Intercepts Scale Events\n",
        "\n",
        "Because the `private-ai` namespace has `kueue.openshift.io/managed=true`:\n",
        "\n",
        "1. Scale command adjusts Deployment replicas â†’ triggers pod creation\n",
        "2. Kueue intercepts via **SchedulingGates** (pods start as `SchedulingGated`)\n",
        "3. Kueue checks against `ClusterQueue` quota (5 GPUs)\n",
        "4. **If under quota:** Pod admitted immediately, starts running\n",
        "5. **If over quota:** Pod held in `SchedulingGated` until resources free\n",
        "\n",
        "### Documentation Links\n",
        "\n",
        "- [RHOAI 3.0: Deploying Models](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0/html-single/deploying_models/index)\n",
        "- [RHOAI 3.0: Managing Distributed Workloads](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0/html-single/working_with_distributed_workloads/index)\n",
        "- [Kueue Documentation](https://kueue.sigs.k8s.io/)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
