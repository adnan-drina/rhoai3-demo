{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéõÔ∏è GPU Switchboard: Dynamic Resource Handover\n",
        "\n",
        "This notebook demonstrates **RHOAI 3.0's GPU-as-a-Service** capabilities using Kueue for dynamic resource allocation.\n",
        "\n",
        "## The Demo Scenario\n",
        "\n",
        "Our cluster has **5 GPUs** available:\n",
        "- 1x `g6.12xlarge` ‚Üí 4 GPUs\n",
        "- 1x `g6.4xlarge` ‚Üí 1 GPU\n",
        "\n",
        "**Baseline State (Full Saturation):**\n",
        "- ‚úÖ `mistral-3-bf16` ‚Üí 4 GPUs (ON)\n",
        "- ‚úÖ `mistral-3-int4` ‚Üí 1 GPU (ON)\n",
        "- ‚è∏Ô∏è `devstral-2` ‚Üí 4 GPUs (OFF, waiting)\n",
        "\n",
        "**The Challenge:** What happens when we need to start Devstral-2 but all GPUs are in use?\n",
        "\n",
        "**The Answer:** Kueue's intelligent queuing holds the request until resources become available!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Import Libraries and Check Cluster State\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import json\n",
        "from IPython.display import display, HTML, clear_output\n",
        "import ipywidgets as widgets\n",
        "import time\n",
        "\n",
        "NAMESPACE = \"private-ai\"\n",
        "\n",
        "# Model configuration\n",
        "MODELS = {\n",
        "    \"mistral-3-bf16\": {\"gpus\": 4, \"color\": \"#2196F3\", \"desc\": \"BF16 Full Precision\"},\n",
        "    \"mistral-3-int4\": {\"gpus\": 1, \"color\": \"#4CAF50\", \"desc\": \"INT4 Quantized\"},\n",
        "    \"devstral-2\": {\"gpus\": 4, \"color\": \"#FF9800\", \"desc\": \"Agentic Model\"}\n",
        "}\n",
        "\n",
        "def run_oc(args):\n",
        "    \"\"\"Execute oc command and return output\"\"\"\n",
        "    result = subprocess.run([\"oc\"] + args, capture_output=True, text=True)\n",
        "    return result.stdout.strip(), result.returncode\n",
        "\n",
        "def get_model_status(name):\n",
        "    \"\"\"Get current minReplicas and ready state for a model\"\"\"\n",
        "    out, _ = run_oc([\"get\", \"inferenceservice\", name, \"-n\", NAMESPACE, \"-o\", \"json\"])\n",
        "    if out:\n",
        "        data = json.loads(out)\n",
        "        min_replicas = data.get(\"spec\", {}).get(\"predictor\", {}).get(\"minReplicas\", 0)\n",
        "        ready = data.get(\"status\", {}).get(\"conditions\", [{}])[-1].get(\"status\", \"Unknown\")\n",
        "        return min_replicas, ready\n",
        "    return 0, \"Unknown\"\n",
        "\n",
        "def set_model_replicas(name, replicas):\n",
        "    \"\"\"Set minReplicas for a model\"\"\"\n",
        "    patch = json.dumps({\"spec\": {\"predictor\": {\"minReplicas\": replicas}}})\n",
        "    out, code = run_oc([\"patch\", \"inferenceservice\", name, \"-n\", NAMESPACE, \n",
        "                        \"--type=merge\", \"-p\", patch])\n",
        "    return code == 0\n",
        "\n",
        "print(\"‚úÖ Setup complete. Connected to namespace:\", NAMESPACE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Current Cluster Status\n",
        "\n",
        "Let's check the current GPU allocation and model states:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_cluster_status():\n",
        "    \"\"\"Display current cluster status\"\"\"\n",
        "    total_gpus = 5\n",
        "    used_gpus = 0\n",
        "    \n",
        "    html = \"<h3>üìä Cluster Status</h3>\"\n",
        "    html += \"<table style='width:100%; border-collapse: collapse;'>\"\n",
        "    html += \"<tr style='background:#f5f5f5;'><th>Model</th><th>GPUs</th><th>State</th><th>Ready</th></tr>\"\n",
        "    \n",
        "    for name, config in MODELS.items():\n",
        "        min_rep, ready = get_model_status(name)\n",
        "        state = \"üü¢ ON\" if min_rep > 0 else \"‚ö´ OFF\"\n",
        "        ready_icon = \"‚úÖ\" if ready == \"True\" else (\"‚è≥\" if min_rep > 0 else \"‚Äî\")\n",
        "        \n",
        "        if min_rep > 0:\n",
        "            used_gpus += config[\"gpus\"]\n",
        "        \n",
        "        html += f\"<tr style='border-bottom:1px solid #ddd;'>\"\n",
        "        html += f\"<td style='padding:8px;'><b style='color:{config['color']}'>{name}</b><br>\"\n",
        "        html += f\"<small>{config['desc']}</small></td>\"\n",
        "        html += f\"<td style='padding:8px; text-align:center;'>{config['gpus']}</td>\"\n",
        "        html += f\"<td style='padding:8px; text-align:center;'>{state}</td>\"\n",
        "        html += f\"<td style='padding:8px; text-align:center;'>{ready_icon}</td>\"\n",
        "        html += \"</tr>\"\n",
        "    \n",
        "    html += \"</table>\"\n",
        "    \n",
        "    # GPU usage bar\n",
        "    pct = (used_gpus / total_gpus) * 100\n",
        "    bar_color = \"#4CAF50\" if pct < 80 else (\"#FF9800\" if pct < 100 else \"#f44336\")\n",
        "    html += f\"<h3>üîã GPU Quota: {used_gpus}/{total_gpus}</h3>\"\n",
        "    html += f\"<div style='background:#e0e0e0; border-radius:10px; height:30px; width:100%;'>\"\n",
        "    html += f\"<div style='background:{bar_color}; width:{pct}%; height:100%; border-radius:10px; \"\n",
        "    html += f\"text-align:center; line-height:30px; color:white; font-weight:bold;'>{pct:.0f}%</div></div>\"\n",
        "    \n",
        "    display(HTML(html))\n",
        "\n",
        "show_cluster_status()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéõÔ∏è GPU Switchboard\n",
        "\n",
        "Use these toggles to control which models are running. Watch how Kueue manages the GPU quota!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create toggle switches for each model\n",
        "output = widgets.Output()\n",
        "\n",
        "def create_toggle(name, config):\n",
        "    min_rep, _ = get_model_status(name)\n",
        "    \n",
        "    toggle = widgets.ToggleButton(\n",
        "        value=(min_rep > 0),\n",
        "        description=f\"{name} ({config['gpus']} GPU)\",\n",
        "        button_style='success' if min_rep > 0 else '',\n",
        "        layout=widgets.Layout(width='280px', height='50px'),\n",
        "        style={'font_weight': 'bold'}\n",
        "    )\n",
        "    \n",
        "    def on_toggle(change):\n",
        "        with output:\n",
        "            clear_output(wait=True)\n",
        "            new_replicas = 1 if change['new'] else 0\n",
        "            action = \"Enabling\" if new_replicas else \"Disabling\"\n",
        "            print(f\"üîÑ {action} {name}...\")\n",
        "            \n",
        "            if set_model_replicas(name, new_replicas):\n",
        "                toggle.button_style = 'success' if new_replicas else ''\n",
        "                print(f\"‚úÖ {name} set to minReplicas={new_replicas}\")\n",
        "                \n",
        "                # Check for queueing\n",
        "                if new_replicas > 0:\n",
        "                    time.sleep(3)\n",
        "                    wl_out, _ = run_oc([\"get\", \"pods\", \"-n\", NAMESPACE, \n",
        "                                        \"-l\", f\"serving.kserve.io/inferenceservice={name}\"])\n",
        "                    if \"SchedulingGated\" in str(wl_out) or \"Pending\" in str(wl_out):\n",
        "                        print(f\"\\n‚ö†Ô∏è  {name} is PENDING in Kueue queue!\")\n",
        "                        print(\"   ‚Üí Not enough GPUs available\")\n",
        "                        print(\"   ‚Üí Disable another model to free resources\")\n",
        "            else:\n",
        "                print(f\"‚ùå Failed to update {name}\")\n",
        "            \n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            show_cluster_status()\n",
        "    \n",
        "    toggle.observe(on_toggle, names='value')\n",
        "    return toggle\n",
        "\n",
        "# Create toggles\n",
        "toggles = [create_toggle(name, config) for name, config in MODELS.items()]\n",
        "\n",
        "# Layout\n",
        "header = widgets.HTML(\"<h2>üéõÔ∏è GPU Switchboard</h2><p>Toggle models ON/OFF to manage GPU allocation</p>\")\n",
        "toggle_box = widgets.VBox(toggles, layout=widgets.Layout(padding='10px'))\n",
        "\n",
        "display(header)\n",
        "display(toggle_box)\n",
        "display(widgets.HTML(\"<hr><h3>üìã Operation Log</h3>\"))\n",
        "display(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Kueue Workload Status\n",
        "\n",
        "Monitor the Kueue queue to see pending workloads:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_kueue_status():\n",
        "    \"\"\"Show Kueue workload queue status\"\"\"\n",
        "    print(\"üîç Kueue Workloads in private-ai namespace:\\n\")\n",
        "    out, _ = run_oc([\"get\", \"workload\", \"-n\", NAMESPACE, \"-o\", \"wide\"])\n",
        "    print(out if out else \"No workloads found\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"\\nüè¢ ClusterQueue Status:\\n\")\n",
        "    out, _ = run_oc([\"get\", \"clusterqueue\", \"rhoai-main-queue\", \"-o\", \"wide\"])\n",
        "    print(out if out else \"ClusterQueue not found\")\n",
        "\n",
        "show_kueue_status()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üé¨ Demo Script: The Resource Handover\n",
        "\n",
        "Follow these steps to demonstrate dynamic GPU allocation:\n",
        "\n",
        "### Step 1: Verify Baseline (100% Saturation)\n",
        "```\n",
        "mistral-3-bf16 (4 GPU) = ON  ‚úÖ\n",
        "mistral-3-int4 (1 GPU) = ON  ‚úÖ\n",
        "devstral-2 (4 GPU)     = OFF ‚è∏Ô∏è\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "Total: 5/5 GPUs used (100%)\n",
        "```\n",
        "\n",
        "### Step 2: Attempt to Start Devstral-2\n",
        "1. Toggle **devstral-2** to ON\n",
        "2. Observe: Kueue puts it in **PENDING** state\n",
        "3. Why? `4 + 1 + 4 = 9 > 5` (over quota)\n",
        "\n",
        "### Step 3: The Handover\n",
        "1. Toggle **mistral-3-bf16** to OFF\n",
        "2. Watch: Devstral-2 **INSTANTLY** starts\n",
        "3. New state: `0 + 1 + 4 = 5` (exactly at quota)\n",
        "\n",
        "### The Message\n",
        "> \"This is GPU-as-a-Service. The platform dynamically manages resources, \n",
        "> ensuring fair access and preventing any single team from hoarding GPUs.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick refresh of all states\n",
        "print(\"üîÑ Refreshing status...\\n\")\n",
        "show_cluster_status()\n",
        "print(\"\\n\")\n",
        "show_kueue_status()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
