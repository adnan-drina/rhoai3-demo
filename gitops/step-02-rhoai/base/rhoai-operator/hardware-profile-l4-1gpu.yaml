# HardwareProfile - NVIDIA L4 1GPU (AWS g6.4xlarge)
# Global profile with Queue-based scheduling for Kueue integration
# Node specs: 16 vCPU, 64 GB RAM, 1x NVIDIA L4 GPU
#
# Uses standardized 'default' LocalQueue name (RHOAI 3.0 best practice)
# Each namespace defines its own 'default' LocalQueue → ClusterQueue mapping
apiVersion: infrastructure.opendatahub.io/v1
kind: HardwareProfile
metadata:
  name: nvidia-l4-1gpu
  namespace: redhat-ods-applications
  annotations:
    argocd.argoproj.io/sync-wave: "16"
    argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true
    opendatahub.io/display-name: "NVIDIA L4 1GPU (g6.4xlarge)"
    opendatahub.io/description: "AWS g6.4xlarge - Single L4 GPU for inference"
    opendatahub.io/disabled: "false"
  labels:
    app.kubernetes.io/part-of: hardwareprofile
    app.opendatahub.io/hardwareprofile: "true"
spec:
  # Queue-based scheduling - uses 'default' LocalQueue in each namespace
  # nodeSelector and tolerations are injected via namespace's LocalQueue → ClusterQueue → ResourceFlavor
  scheduling:
    type: Queue
    kueue:
      localQueueName: default
  identifiers:
    - identifier: cpu
      displayName: CPU
      resourceType: CPU
      minCount: 1
      maxCount: 14
      defaultCount: 8
    - identifier: memory
      displayName: Memory
      resourceType: Memory
      minCount: 4Gi
      maxCount: 60Gi
      defaultCount: 32Gi
    - identifier: nvidia.com/gpu
      displayName: NVIDIA L4 GPU
      resourceType: Accelerator
      minCount: 1
      maxCount: 1
      defaultCount: 1
