apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

# Step 07: Model Performance Metrics - "The ROI of Quantization"
# ============================================================================
# Demonstrates why Inference Efficiency is the most important metric for
# enterprise AI deployments.
#
# The Narrative:
#   - Baseline: Mistral-3-BF16 sharded across 4 GPUs ($$$$)
#   - Competitor: Mistral-3-INT4 running on 1 GPU ($)
#   - Goal: Find the "Efficiency Delta" - where does each model saturate?
#
# Components:
#   1. OpenShift Pipelines Operator (pipelines-operator/)
#      - Red Hat OpenShift Pipelines (Tekton) for CI/CD pipelines
#      - Channel: latest, Version: 1.20.2
#      - Enables self-service GuideLLM benchmark runs via Pipelines
#
#   2. Grafana Operator (grafana-operator/)
#      - Installs community Grafana Operator from OperatorHub
#      - Deploys Grafana instance in private-ai namespace
#      - Official vLLM Performance Statistics dashboard (auto-updated from GitHub)
#      - DCGM GPU Metrics dashboard
#      - Mistral 3 (BF16 vs INT4) comparison dashboard
#
#   3. GuideLLM Benchmarking (guidellm/)
#      - CronJob for daily automated benchmarks
#      - Poisson distribution for realistic human traffic simulation
#      - Rate sweep from 0.1 to 5.0 req/s
#
#   4. GuideLLM Pipeline (guidellm-pipeline/)
#      - Tekton Task and Pipeline for self-service benchmarks
#      - Pre-configured PipelineRuns for each model
#      - Results stored in persistent volume
#
# Dashboards (managed by Grafana Operator):
#   - vLLM Performance Statistics (official from vLLM GitHub)
#   - vLLM Query Statistics (official from vLLM GitHub)
#   - NVIDIA DCGM GPU Metrics
#   - Mistral 3 (BF16 vs INT4) - ROI of Quantization
#
# SLA Targets:
#   - TTFT: < 500ms (excellent), < 1.0s (acceptable), > 2.0s (breaking)
#   - TPOT: > 50 tok/s (excellent), > 20 tok/s (acceptable), < 10 tok/s (breaking)
#   - KV Cache: < 70% (excellent), 70-85% (acceptable), > 95% (breaking)
#   - Queue Depth: 0 (excellent), 0-1 (acceptable), > 5 (breaking)
#
# For interactive benchmark UI, see Step 07B (vLLM-Playground - future).
#
# Ref: https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0/html/managing_and_monitoring_models/index
# Ref: https://docs.redhat.com/en/documentation/red_hat_openshift_pipelines/1.20
# Ref: https://docs.vllm.ai/en/latest/serving/metrics.html
# Ref: https://github.com/neuralmagic/guidellm
# Ref: https://github.com/rh-aiservices-bu/guidellm-pipeline
# Ref: https://github.com/grafana/grafana-operator
# Ref: https://github.com/redhat-cop/gitops-catalog/tree/main/openshift-pipelines-operator
# Ref: https://github.com/redhat-cop/gitops-catalog/tree/main/grafana-operator
# ============================================================================

resources:
  # OpenShift Pipelines Operator (Tekton) for self-service benchmarks
  - pipelines-operator/
  
  # Grafana Operator: Operator + Instance + Dashboards as CRs
  - grafana-operator/
  
  # GuideLLM automated benchmarking (CronJob for daily runs)
  - guidellm/
  
  # GuideLLM Pipeline (Tekton Task/Pipeline for self-service runs)
  - guidellm-pipeline/
  
  # GuideLLM Workbench (Streamlit UI for interactive benchmarking)
  - guidellm-workbench/

# Note: Removed commonLabels to avoid Service selector mismatch issues.
# Labels are applied directly in individual manifests instead.
