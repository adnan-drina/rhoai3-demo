apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

# Step 07: Model Performance Metrics - "The ROI of Quantization"
# ============================================================================
# Demonstrates why Inference Efficiency is the most important metric for
# enterprise AI deployments.
#
# The Narrative:
#   - Baseline: Mistral-3-BF16 sharded across 4 GPUs ($$$$)
#   - Competitor: Mistral-3-INT4 running on 1 GPU ($)
#   - Goal: Find the "Efficiency Delta" - where does each model saturate?
#
# Components:
#   1. Grafana Operator (grafana-operator/)
#      - Installs community Grafana Operator from OperatorHub
#      - Deploys Grafana instance in private-ai namespace
#      - Official vLLM Performance Statistics dashboard (auto-updated from GitHub)
#      - DCGM GPU Metrics dashboard
#      - Mistral ROI Comparison dashboard
#
#   2. GuideLLM Benchmarking (guidellm/)
#      - Poisson distribution for realistic human traffic simulation
#      - Rate sweep from 0.1 to 5.0 req/s
#      - ROI comparison scripts (INT4 vs BF16)
#
# Dashboards (managed by Grafana Operator):
#   - vLLM Performance Statistics (official from vLLM GitHub)
#   - vLLM Query Statistics (official from vLLM GitHub)
#   - NVIDIA DCGM GPU Metrics
#   - Mistral ROI Comparison (BF16 vs INT4)
#
# SLA Targets:
#   - TTFT: < 500ms (excellent), < 1.0s (acceptable)
#   - TPOT: > 20 tokens/sec
#   - KV Cache: < 85% (warning at 95%)
#
# For interactive benchmark UI, see Step 07B (vLLM-Playground - future).
#
# Ref: https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0/html/managing_and_monitoring_models/index
# Ref: https://docs.vllm.ai/en/latest/serving/metrics.html
# Ref: https://github.com/neuralmagic/guidellm
# Ref: https://github.com/grafana/grafana-operator
# Ref: https://github.com/redhat-cop/gitops-catalog/tree/main/grafana-operator
# ============================================================================

resources:
  # Grafana Operator: Operator + Instance + Dashboards as CRs
  - grafana-operator/
  
  # GuideLLM automated benchmarking with Poisson distribution
  - guidellm/

# Note: Removed commonLabels to avoid Service selector mismatch issues.
# Labels are applied directly in individual manifests instead.
