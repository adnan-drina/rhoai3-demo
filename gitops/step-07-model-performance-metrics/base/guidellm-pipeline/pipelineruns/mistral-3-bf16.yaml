# GuideLLM Benchmark - Mistral-3 BF16 (4-GPU)
# ============================================================================
# Full-precision Mistral-3 model benchmark.
#
# Model: mistral-3-bf16
# GPUs: 4 x NVIDIA L4 (Tensor Parallelism)
# Precision: BF16 (full precision)
#
# Usage:
#   oc create -f mistral-3-bf16.yaml
#   # or
#   tkn pipeline start guidellm-benchmark \
#     --param model-name=mistral-3-bf16 \
#     --workspace name=results,claimName=guidellm-pipeline-results
# ============================================================================
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  generateName: benchmark-mistral-3-bf16-
  namespace: private-ai
  labels:
    app: guidellm-pipeline
    model: mistral-3-bf16
    app.kubernetes.io/part-of: benchmarking
spec:
  pipelineRef:
    name: guidellm-benchmark
  
  params:
    - name: model-name
      value: "mistral-3-bf16"
    - name: profile
      value: "sweep"
    - name: max-seconds
      value: "60"
    - name: max-requests
      value: "50"
  
  workspaces:
    # Use emptyDir to avoid affinity-assistant (which may land on GPU nodes)
    # Results are printed to logs; use CronJob + PVC for persistent results
    - name: results
      emptyDir: {}
  
  # Use CPU-only LocalQueue (points to 'default' ClusterQueue with default-flavor)
  # This ensures pipeline pods are scheduled on non-GPU nodes
  taskRunSpecs:
    - pipelineTaskName: benchmark
      metadata:
        labels:
          kueue.x-k8s.io/queue-name: cpu-workloads

