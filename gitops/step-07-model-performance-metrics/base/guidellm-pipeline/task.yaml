# GuideLLM Benchmark Task - Flexible Load Testing
# ============================================================================
# Tekton Task for running GuideLLM benchmarks against vLLM models.
#
# Supports two data modes:
#   1. prompts: Use simple prompts (quick tests, variable load)
#   2. synthetic: Use synthetic data with controlled token counts (recommended)
#
# Profiles:
#   - sweep: Automatic rate exploration (finds baseline, optimal, breaking point)
#   - poisson: Realistic user simulation (Poisson-distributed arrivals)
#   - constant: Fixed request rate (for breaking point analysis)
#   - throughput: Maximum throughput test
#   - synchronous: One request at a time (baseline)
#
# SLA Thresholds:
#   TTFT: < 1.0s (SLA), > 2.0s (Breaking)
#   TPOT: < 50ms (SLA), > 100ms (Breaking)
#   KV Cache: < 95% (healthy)
#
# Ref: https://github.com/vllm-project/guidellm
# ============================================================================
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: guidellm-benchmark
  namespace: private-ai
  labels:
    app: guidellm-pipeline
    app.kubernetes.io/part-of: benchmarking
  annotations:
    argocd.argoproj.io/sync-wave: "10"
    argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true
spec:
  description: |
    Run GuideLLM benchmark against a vLLM model endpoint.
    Generates comprehensive performance metrics including TTFT, TPOT, and throughput.
  
  params:
    - name: model-name
      type: string
      description: Name of the model to benchmark (e.g., mistral-3-int4)
    
    - name: target-url
      type: string
      description: Model endpoint URL (leave empty for auto-discovery)
      default: ""
    
    - name: profile
      type: string
      description: |
        Benchmark profile:
          - sweep: Automatic exploration (recommended for daily benchmarks)
          - poisson: Realistic user load (default)
          - constant: Fixed request rate
          - throughput: Maximum capacity test
          - synchronous: Baseline (one at a time)
      default: "sweep"
    
    - name: rate
      type: string
      description: |
        For sweep: number of benchmark iterations (e.g., "5")
        For poisson/constant: requests per second (e.g., "10")
      default: "5"
    
    - name: data-type
      type: string
      description: |
        Data source type:
          - synthetic: Controlled token counts (recommended)
          - prompts: Simple prompts file
      default: "synthetic"
    
    - name: prompt-tokens
      type: string
      description: Input token count for synthetic data
      default: "256"
    
    - name: output-tokens
      type: string
      description: Output token count for synthetic data
      default: "256"
    
    - name: processor
      type: string
      description: HuggingFace tokenizer for synthetic data generation
      default: "mistralai/Mistral-Small-3.1-24B-Instruct-2503"
    
    - name: max-seconds
      type: string
      description: Maximum seconds per benchmark strategy
      default: "180"
    
    - name: max-requests
      type: string
      description: Maximum requests per benchmark strategy
      default: "150"
  
  workspaces:
    - name: results
      description: Workspace for storing benchmark results
  
  results:
    - name: output-file
      description: Path to the benchmark results JSON file
    - name: status
      description: Benchmark completion status (success/failed)
  
  steps:
    - name: run-benchmark
      image: ghcr.io/vllm-project/guidellm:stable
      env:
        - name: HOME
          value: /tmp
        - name: HF_HOME
          value: /tmp/.cache/huggingface
      script: |
        #!/bin/bash
        set -e
        
        MODEL_NAME="$(params.model-name)"
        PROFILE="$(params.profile)"
        RATE="$(params.rate)"
        DATA_TYPE="$(params.data-type)"
        PROMPT_TOKENS="$(params.prompt-tokens)"
        OUTPUT_TOKENS="$(params.output-tokens)"
        PROCESSOR="$(params.processor)"
        MAX_SECONDS="$(params.max-seconds)"
        MAX_REQUESTS="$(params.max-requests)"
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        OUTPUT_DIR="$(workspaces.results.path)/${MODEL_NAME}"
        OUTPUT_FILE="${OUTPUT_DIR}/${MODEL_NAME}-${TIMESTAMP}.json"
        
        # Determine target URL
        if [ -n "$(params.target-url)" ]; then
          TARGET="$(params.target-url)"
        else
          TARGET="http://${MODEL_NAME}-predictor.private-ai.svc.cluster.local:8080/v1"
        fi
        
        # ============================================================
        # FIX: Resolve DNS to IP before running GuideLLM
        # GuideLLM uses Python multiprocessing which forks workers.
        # Forked workers cannot resolve DNS (known container issue).
        # ============================================================
        HOSTNAME=$(echo "${TARGET}" | sed -E 's|https?://([^:/]+).*|\1|')
        echo "Resolving DNS for: ${HOSTNAME}"
        
        IP=$(getent hosts "${HOSTNAME}" 2>/dev/null | awk '{print $1}' | head -1)
        
        if [ -z "${IP}" ]; then
          echo "Warning: Could not resolve ${HOSTNAME} via getent, trying nslookup..."
          IP=$(nslookup "${HOSTNAME}" 2>/dev/null | grep -A1 'Name:' | grep 'Address:' | awk '{print $2}' | head -1)
        fi
        
        if [ -z "${IP}" ]; then
          echo "Warning: DNS resolution failed, using original URL"
        else
          TARGET=$(echo "${TARGET}" | sed "s|${HOSTNAME}|${IP}|")
          echo "✓ Resolved to: ${TARGET}"
        fi
        
        echo "╔═══════════════════════════════════════════════════════════════════╗"
        echo "║  GuideLLM Benchmark Pipeline                                      ║"
        echo "╠═══════════════════════════════════════════════════════════════════╣"
        echo "║  Model:       ${MODEL_NAME}"
        echo "║  Target:      ${TARGET}"
        echo "║  Profile:     ${PROFILE}"
        echo "║  Rate:        ${RATE}"
        echo "║  Data:        ${DATA_TYPE}"
        if [ "${DATA_TYPE}" = "synthetic" ]; then
        echo "║  Tokens:      ${PROMPT_TOKENS} input, ${OUTPUT_TOKENS} output"
        echo "║  Processor:   ${PROCESSOR}"
        fi
        echo "║  Duration:    ${MAX_SECONDS}s / ${MAX_REQUESTS} requests"
        echo "╚═══════════════════════════════════════════════════════════════════╝"
        echo ""
        
        # Create output directory
        mkdir -p "${OUTPUT_DIR}"
        
        # Check if target is reachable
        echo "Checking model availability..."
        if ! curl -s --connect-timeout 30 "${TARGET}/models" >/dev/null 2>&1; then
          echo "❌ ERROR: Model ${MODEL_NAME} is not reachable at ${TARGET}"
          echo "failed" > $(results.status.path)
          exit 1
        fi
        echo "✓ Model is reachable"
        echo ""
        
        # Build GuideLLM command based on data type
        if [ "${DATA_TYPE}" = "synthetic" ]; then
          # Synthetic data with controlled token counts
          DATA_ARGS="--data \"prompt_tokens=${PROMPT_TOKENS},output_tokens=${OUTPUT_TOKENS}\" --processor \"${PROCESSOR}\""
        else
          # Simple prompts file
          cat > /tmp/prompts.json << 'EOF'
        [
          {"prompt": "What is the capital of France?"},
          {"prompt": "Explain quantum computing in simple terms."},
          {"prompt": "Write a short poem about the ocean."},
          {"prompt": "What are the benefits of exercise?"},
          {"prompt": "How does photosynthesis work?"},
          {"prompt": "Describe the process of making bread."},
          {"prompt": "What is machine learning?"},
          {"prompt": "Tell me about the solar system."},
          {"prompt": "What causes rain?"},
          {"prompt": "Explain the theory of relativity."},
          {"prompt": "What is the difference between AI and machine learning?"},
          {"prompt": "How do neural networks work?"},
          {"prompt": "Explain the concept of cloud computing."},
          {"prompt": "What are the principles of object-oriented programming?"},
          {"prompt": "Describe the water cycle in detail."}
        ]
        EOF
          DATA_ARGS="--data /tmp/prompts.json"
        fi
        
        # Run benchmark
        echo "Starting ${PROFILE} benchmark..."
        echo ""
        
        eval guidellm benchmark run \
          --target "${TARGET}" \
          ${DATA_ARGS} \
          --profile "${PROFILE}" \
          --rate "${RATE}" \
          --max-seconds "${MAX_SECONDS}" \
          --max-requests "${MAX_REQUESTS}" \
          --output-dir "${OUTPUT_DIR}" \
          --disable-console-interactive \
          2>&1
        
        RESULT=$?
        
        echo ""
        echo "═══════════════════════════════════════════════════════════════════"
        if [ $RESULT -eq 0 ]; then
          echo "  ✓ Benchmark completed successfully!"
          echo "success" > $(results.status.path)
        else
          echo "  ⚠️ Benchmark completed with warnings (exit code: $RESULT)"
          echo "success" > $(results.status.path)
        fi
        echo "═══════════════════════════════════════════════════════════════════"
        
        # Write output file path to results
        echo "${OUTPUT_FILE}" > $(results.output-file.path)
        
        # List results
        echo ""
        echo "Results directory:"
        ls -la "${OUTPUT_DIR}" 2>/dev/null | tail -10 || echo "No files yet"
      
      securityContext:
        runAsNonRoot: true
      
      computeResources:
        requests:
          cpu: 500m
          memory: 1Gi
        limits:
          cpu: "2"
          memory: 4Gi
