# GuideLLM Benchmark Configuration
# ============================================================================
# Contains benchmark scripts for finding model "breaking points" using
# Poisson distribution to simulate realistic human traffic patterns.
#
# The "ROI of Quantization" Narrative:
#   - Compare BF16 (4-GPU $$$$) vs INT4 (1-GPU $)
#   - Find the "Efficiency Delta" - where does each model saturate?
#   - Calculate: How many 1-GPU specialists = one 4-GPU powerhouse?
#
# Methodology: Poisson Stress Test
#   - Uses Poisson arrival process (natural human variability)
#   - Rate sweep from 0.1 to 5.0 requests/second
#   - SLA Targets: TTFT < 1.0s (acceptable), < 500ms (excellent)
#
# Ref: https://github.com/neuralmagic/guidellm
# Ref: https://developers.redhat.com/articles/2025/06/20/guidellm-evaluate-llm-deployments-real-world-inference
# ============================================================================
apiVersion: v1
kind: ConfigMap
metadata:
  name: guidellm-scripts
  namespace: private-ai
  labels:
    app: guidellm
    app.kubernetes.io/part-of: step-07-model-performance-metrics
  annotations:
    argocd.argoproj.io/sync-wave: "1"
data:
  benchmark-sweep.sh: |
    #!/bin/bash
    # GuideLLM Benchmark Sweep Script
    # ============================================================================
    # Uses Poisson distribution for realistic human traffic simulation.
    # Sweeps request rates from 0.1 to 5.0 req/s to find saturation points.
    #
    # The "ROI of Quantization" Test:
    #   - INT4 (1-GPU): Lower cost, earlier saturation
    #   - BF16 (4-GPU): Higher cost, more headroom
    #   - Goal: Find the "Efficiency Delta"
    # ============================================================================
    
    set -e
    
    RESULTS_DIR="/results"
    TIMESTAMP=$(date +%Y%m%d_%H%M%S)
    
    # SLA Thresholds
    TTFT_TARGET_EXCELLENT="0.5"  # 500ms
    TTFT_TARGET_ACCEPTABLE="1.0" # 1 second
    TPOT_TARGET="20"             # 20 tokens/sec
    
    # Models to benchmark (internal ClusterIP URLs)
    MODELS=""
    
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "  GuideLLM Poisson Stress Test"
    echo "  The 'ROI of Quantization' Benchmark"
    echo "  Started: $(date)"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    echo "  ğŸ“Š SLA Targets:"
    echo "     TTFT (Excellent): < ${TTFT_TARGET_EXCELLENT}s"
    echo "     TTFT (Acceptable): < ${TTFT_TARGET_ACCEPTABLE}s"
    echo "     TPOT: > ${TPOT_TARGET} tokens/sec"
    echo ""
    
    # Discover available models
    echo "â–¶ Discovering models..."
    
    # Priority order: Compare INT4 vs BF16 first (the main story)
    # Note: Using port 8080 because headless services resolve directly to pod IPs
    for model in mistral-3-int4 mistral-3-bf16 granite-8b-agent devstral-2 gpt-oss-20b; do
      URL="http://${model}-predictor.private-ai.svc.cluster.local:8080"
      if curl -s --connect-timeout 5 "${URL}/v1/models" >/dev/null 2>&1; then
        echo "  âœ“ ${model} is available"
        MODELS="${MODELS} ${model}"
      else
        echo "  âš ï¸  ${model} is not responding (skipping)"
      fi
    done
    
    if [ -z "${MODELS}" ]; then
      echo ""
      echo "âŒ No models available for benchmarking"
      exit 1
    fi
    
    # Benchmark Configuration
    # Using 'sweep' profile for automatic rate discovery
    # This tests throughput from low to saturation point
    SWEEP_STEPS="5"        # Number of rate points to test
    MAX_SECONDS="60"       # Duration per test point
    
    echo ""
    echo "â–¶ Running Sweep Benchmarks..."
    echo "  Profile: sweep (automatic rate discovery)"
    echo "  Steps: ${SWEEP_STEPS} rate points per scenario"
    echo "  Duration: ${MAX_SECONDS}s per step"
    
    for model in ${MODELS}; do
      MODEL_URL="http://${model}-predictor.private-ai.svc.cluster.local:8080"
      MODEL_RESULTS_DIR="${RESULTS_DIR}/${TIMESTAMP}/${model}"
      mkdir -p "${MODEL_RESULTS_DIR}"
      
      echo ""
      echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
      echo "  Benchmarking: ${model}"
      echo "  URL: ${MODEL_URL}"
      echo "  Strategy: Find the 'Breaking Point' (TTFT > ${TTFT_TARGET_ACCEPTABLE}s)"
      echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
      
      # Using HuggingFace dataset with explicit tokenizer
      # Note: Requires network access to download dataset/tokenizer
      
      # Scenario: Sweep Test - Find saturation point
      echo ""
      echo "  ğŸ“Š Sweep Test (Find Saturation Point)"
      echo "     Profile: sweep - automatically finds max throughput"
      guidellm benchmark run \
        --target "${MODEL_URL}/v1" \
        --model "${model}" \
        --data "openai/gsm8k" \
        --processor "mistralai/Mistral-7B-v0.1" \
        --profile sweep \
        --rate ${SWEEP_STEPS} \
        --max-seconds ${MAX_SECONDS} \
        --max-requests 50 \
        --output-dir "${MODEL_RESULTS_DIR}" \
        --outputs "sweep-test.json" \
        --disable-console-interactive \
        2>&1 || echo "  âš ï¸  Sweep benchmark failed"
      
      echo ""
      echo "  âœ“ Completed all scenarios for ${model}"
    done
    
    # Summary with Efficiency Analysis
    echo ""
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "  Benchmark Sweep Complete!"
    echo "  Results saved to: ${RESULTS_DIR}/${TIMESTAMP}/"
    echo "  Finished: $(date)"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    echo "â–¶ Results Summary:"
    find "${RESULTS_DIR}/${TIMESTAMP}" -name "*.json" -exec echo "  - {}" \;
    echo ""
    echo "â–¶ Next Steps:"
    echo "  1. View results in Grafana dashboard"
    echo "  2. Compare TTFT curves: INT4 vs BF16"
    echo "  3. Find the 'Knee of the Curve' (saturation point)"
    echo "  4. Calculate: Tokens-per-GPU efficiency"
    
  efficiency-comparison.sh: |
    #!/bin/bash
    # ROI of Quantization: Direct INT4 vs BF16 Comparison
    # ============================================================================
    # Runs identical workloads on both models to measure the "Efficiency Delta"
    # 
    # The Question: At what point does the 1-GPU model saturate,
    # and how many 1-GPU specialists can we run for the cost of one 4-GPU?
    # ============================================================================
    
    set -e
    
    RESULTS_DIR="/results"
    TIMESTAMP=$(date +%Y%m%d_%H%M%S)
    COMPARISON_DIR="${RESULTS_DIR}/${TIMESTAMP}/efficiency-comparison"
    mkdir -p "${COMPARISON_DIR}"
    
    # Using port 8080 because headless services resolve directly to pod IPs
    INT4_URL="http://mistral-3-int4-predictor.private-ai.svc.cluster.local:8080"
    BF16_URL="http://mistral-3-bf16-predictor.private-ai.svc.cluster.local:8080"
    
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "  ROI of Quantization: INT4 vs BF16"
    echo "  The 'Efficiency Delta' Analysis"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    echo "  ğŸ¯ Goal: Find the break-even point"
    echo "     INT4 (1-GPU):  \$   - Lower cost, earlier saturation"
    echo "     BF16 (4-GPU):  \$\$\$\$ - Higher cost, more headroom"
    echo ""
    
    # Check model availability
    INT4_AVAILABLE=false
    BF16_AVAILABLE=false
    
    if curl -s --connect-timeout 5 "${INT4_URL}/v1/models" >/dev/null 2>&1; then
      echo "  âœ“ mistral-3-int4 is available"
      INT4_AVAILABLE=true
    else
      echo "  âŒ mistral-3-int4 is not available"
    fi
    
    if curl -s --connect-timeout 5 "${BF16_URL}/v1/models" >/dev/null 2>&1; then
      echo "  âœ“ mistral-3-bf16 is available"
      BF16_AVAILABLE=true
    else
      echo "  âŒ mistral-3-bf16 is not available"
    fi
    
    if [ "$INT4_AVAILABLE" = false ] || [ "$BF16_AVAILABLE" = false ]; then
      echo ""
      echo "âš ï¸  Both models must be running for comparison"
      exit 1
    fi
    
    # Run identical workloads with sweep profile for throughput discovery
    echo ""
    echo "â–¶ Running sweep benchmarks for ROI comparison..."
    
    # Chat workload (most common use case)
    echo ""
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo "  Scenario: Interactive Chat (the 'Vibe Check')"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    
    echo ""
    echo "  Testing INT4 (1-GPU)..."
    guidellm benchmark run \
      --target "${INT4_URL}/v1" \
      --model "mistral-3-int4" \
      --data '{"prompt_tokens": 128, "output_tokens": 64}' \
      --profile sweep \
      --rate 8 \
      --max-seconds 60 \
      --max-requests 100 \
      --output-dir "${COMPARISON_DIR}" \
      --outputs "int4-sweep.json" \
      --disable-console-interactive \
      2>&1 || echo "  âš ï¸  INT4 benchmark failed"
    
    echo ""
    echo "  Testing BF16 (4-GPU)..."
    guidellm benchmark run \
      --target "${BF16_URL}/v1" \
      --model "mistral-3-bf16" \
      --data '{"prompt_tokens": 128, "output_tokens": 64}' \
      --profile sweep \
      --rate 8 \
      --max-seconds 60 \
      --max-requests 100 \
      --output-dir "${COMPARISON_DIR}" \
      --outputs "bf16-sweep.json" \
      --disable-console-interactive \
      2>&1 || echo "  âš ï¸  BF16 benchmark failed"
    
    echo ""
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "  Efficiency Comparison Complete!"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo ""
    echo "â–¶ Results:"
    echo "  INT4: ${COMPARISON_DIR}/int4-chat.json"
    echo "  BF16: ${COMPARISON_DIR}/bf16-chat.json"
    echo ""
    echo "â–¶ Analysis Questions:"
    echo "  1. At what req/s does INT4's TTFT exceed 1.0s? (Break Point)"
    echo "  2. At what req/s does BF16's TTFT exceed 1.0s? (Break Point)"
    echo "  3. Efficiency Delta = BF16_breakpoint / INT4_breakpoint"
    echo "  4. Cost Analysis: Can we run 4x INT4 instances for the same $ as 1x BF16?"
    
  single-model-benchmark.sh: |
    #!/bin/bash
    # Single model benchmark script using sweep profile
    # Usage: single-model-benchmark.sh <model-name> [scenario] [sweep-steps]
    
    set -e
    
    MODEL_NAME="${1:-mistral-3-int4}"
    SCENARIO="${2:-chat}"
    SWEEP_STEPS="${3:-5}"
    RESULTS_DIR="/results"
    TIMESTAMP=$(date +%Y%m%d_%H%M%S)
    
    # Using port 8080 because headless services resolve directly to pod IPs
    MODEL_URL="http://${MODEL_NAME}-predictor.private-ai.svc.cluster.local:8080"
    OUTPUT_DIR="${RESULTS_DIR}/${TIMESTAMP}_${MODEL_NAME}"
    
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    echo "  GuideLLM Single Model Benchmark"
    echo "  Model: ${MODEL_NAME}"
    echo "  Scenario: ${SCENARIO}"
    echo "  Profile: sweep (automatic throughput discovery)"
    echo "  Steps: ${SWEEP_STEPS}"
    echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
    
    mkdir -p "${OUTPUT_DIR}"
    
    # Check model availability
    if ! curl -s --connect-timeout 10 "${MODEL_URL}/v1/models" >/dev/null 2>&1; then
      echo "âŒ Model ${MODEL_NAME} is not responding"
      exit 1
    fi
    echo "  âœ“ Model is available"
    
    # Set scenario parameters
    case "${SCENARIO}" in
      chat)
        PROMPT_TOKENS=64
        OUTPUT_TOKENS=64
        DESCRIPTION="Interactive chat"
        ;;
      summarization)
        PROMPT_TOKENS=512
        OUTPUT_TOKENS=128
        DESCRIPTION="Document summarization"
        ;;
      code-gen)
        PROMPT_TOKENS=128
        OUTPUT_TOKENS=256
        DESCRIPTION="Code generation"
        ;;
      stress)
        PROMPT_TOKENS=256
        OUTPUT_TOKENS=256
        DESCRIPTION="Stress test (high I/O)"
        ;;
      *)
        echo "Unknown scenario: ${SCENARIO}"
        echo "Available: chat, summarization, code-gen, stress"
        exit 1
        ;;
    esac
    
    echo ""
    echo "â–¶ Running ${DESCRIPTION}..."
    echo "  Using synthetic data: prompt_tokens=${PROMPT_TOKENS}, output_tokens=${OUTPUT_TOKENS}"
    
    guidellm benchmark run \
      --target "${MODEL_URL}/v1" \
      --model "${MODEL_NAME}" \
      --data "{\"prompt_tokens\": ${PROMPT_TOKENS}, \"output_tokens\": ${OUTPUT_TOKENS}}" \
      --profile sweep \
      --rate ${SWEEP_STEPS} \
      --max-seconds 60 \
      --max-requests 100 \
      --output-dir "${OUTPUT_DIR}" \
      --outputs "${SCENARIO}.json" \
      --disable-console-interactive
    
    echo ""
    echo "âœ“ Benchmark complete!"
    echo "  Results: ${OUTPUT_FILE}"

