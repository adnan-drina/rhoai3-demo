# GuideLLM Scheduled Benchmark CronJob
# ============================================================================
# Runs automated benchmark sweeps on a schedule.
# Default: Daily at 2:00 AM UTC
#
# Uses the official GuideLLM container image from:
# https://github.com/vllm-project/guidellm/pkgs/container/guidellm
#
# Results are persisted to the guidellm-results PVC.
#
# To trigger manually:
#   oc create job --from=cronjob/guidellm-daily manual-benchmark -n private-ai
# ============================================================================
apiVersion: batch/v1
kind: CronJob
metadata:
  name: guidellm-daily
  namespace: private-ai
  labels:
    app: guidellm
    app.kubernetes.io/part-of: step-07-model-performance-metrics
  annotations:
    argocd.argoproj.io/sync-wave: "10"
spec:
  # Run daily at 2:00 AM UTC
  schedule: "0 2 * * *"
  concurrencyPolicy: Forbid
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      backoffLimit: 1
      activeDeadlineSeconds: 3600  # 1 hour max
      template:
        metadata:
          labels:
            app: guidellm
            job-type: scheduled
        spec:
          restartPolicy: Never
          # Ensure PVC is writable by any user (OpenShift assigns random UID)
          securityContext:
            fsGroup: 0
          # Note: This job needs network access to download HuggingFace datasets/tokenizers
          # If running in a restricted environment, consider pre-downloading data
          containers:
            - name: guidellm
              # Official GuideLLM container image (stable release v0.5.0)
              # Ref: https://github.com/vllm-project/guidellm/pkgs/container/guidellm
              image: ghcr.io/vllm-project/guidellm:stable
              env:
                # Set HOME to writable directory for cache
                - name: HOME
                  value: /tmp
                # Set HuggingFace cache directory
                - name: HF_HOME
                  value: /tmp/.cache/huggingface
                # GuideLLM Configuration via environment variables
                - name: GUIDELLM_TARGET
                  value: "http://mistral-3-int4-predictor.private-ai.svc.cluster.local:8080/v1"
                - name: GUIDELLM_PROFILE
                  value: "sweep"
                - name: GUIDELLM_MAX_SECONDS
                  value: "60"
                - name: GUIDELLM_MAX_REQUESTS
                  value: "50"
                - name: GUIDELLM_OUTPUT_DIR
                  value: /results
              command:
                - /bin/bash
                - -c
                - |
                  echo "═══════════════════════════════════════════════════════════════"
                  echo "  GuideLLM v0.5.0 - Multi-Model Benchmark"
                  echo "  Profile: ${GUIDELLM_PROFILE}"
                  echo "  Max Seconds: ${GUIDELLM_MAX_SECONDS} per model"
                  echo "═══════════════════════════════════════════════════════════════"
                  echo ""
                  
                  # Create sample prompts (no network needed)
                  cat > /tmp/prompts.json << 'EOF'
                  [
                    {"prompt": "What is the capital of France?"},
                    {"prompt": "Explain quantum computing in simple terms."},
                    {"prompt": "Write a short poem about the ocean."},
                    {"prompt": "What are the benefits of exercise?"},
                    {"prompt": "How does photosynthesis work?"},
                    {"prompt": "Describe the process of making bread."},
                    {"prompt": "What is machine learning?"},
                    {"prompt": "Tell me about the solar system."},
                    {"prompt": "What causes rain?"},
                    {"prompt": "Explain the theory of relativity."}
                  ]
                  EOF
                  echo "✓ Sample prompts created"
                  echo ""
                  
                  # Models to test (BF16 first for ROI comparison)
                  MODELS="mistral-3-bf16 mistral-3-int4"
                  TIMESTAMP=$(date +%Y%m%d_%H%M%S)
                  
                  for MODEL in ${MODELS}; do
                    TARGET="http://${MODEL}-predictor.private-ai.svc.cluster.local:8080/v1"
                    
                    echo "───────────────────────────────────────────────────────────────"
                    echo "  Testing: ${MODEL}"
                    echo "  Target: ${TARGET}"
                    echo "───────────────────────────────────────────────────────────────"
                    
                    # Check if target is reachable
                    if ! curl -s --connect-timeout 10 "${TARGET}/models" >/dev/null 2>&1; then
                      echo "  ⚠️  Model ${MODEL} is not reachable, skipping"
                      continue
                    fi
                    echo "  ✓ Target is reachable"
                    
                    # Run benchmark
                    guidellm benchmark run \
                      --target "${TARGET}" \
                      --data /tmp/prompts.json \
                      --profile "${GUIDELLM_PROFILE}" \
                      --max-seconds "${GUIDELLM_MAX_SECONDS}" \
                      --max-requests "${GUIDELLM_MAX_REQUESTS}" \
                      --output-dir "${GUIDELLM_OUTPUT_DIR}" \
                      --outputs "${MODEL}-${TIMESTAMP}.json" \
                      --disable-console-interactive \
                      2>&1 || echo "  ⚠️  Benchmark for ${MODEL} exited with code $?"
                    
                    echo "  ✓ Completed ${MODEL}"
                    echo ""
                  done
                  
                  echo "═══════════════════════════════════════════════════════════════"
                  echo "  All benchmarks complete"
                  echo "═══════════════════════════════════════════════════════════════"
                  ls -la "${GUIDELLM_OUTPUT_DIR}" 2>/dev/null | tail -10 || true
              volumeMounts:
                - name: results
                  mountPath: /results
                - name: cache
                  mountPath: /tmp/.cache
              resources:
                requests:
                  cpu: 500m
                  memory: 512Mi
                limits:
                  cpu: "2"
                  memory: 2Gi
          volumes:
            - name: results
              persistentVolumeClaim:
                claimName: guidellm-results
            - name: cache
              emptyDir: {}

