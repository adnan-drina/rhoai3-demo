apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-datasources
  namespace: private-ai
  labels:
    app: grafana
data:
  prometheus.yaml: |
    apiVersion: 1
    datasources:
      - name: Prometheus
        type: prometheus
        access: proxy
        # URL for User Workload Monitoring Thanos Querier
        url: https://thanos-querier.openshift-monitoring.svc.cluster.local:9091
        jsonData:
          timeInterval: 5s
          tlsSkipVerify: true
        basicAuth: true
        basicAuthUser: internal
        secureJsonData:
          basicAuthPassword: "" # Token is usually injected or handled via ServiceAccount bearer token
        # For simple demo access within cluster, we often use the service account token.
        # However, connecting to User Workload Monitoring usually requires Bearer Token of a SA with cluster-monitoring-view.
        # For simplicity in this User-Managed Grafana, we will point to the Service directly if possible or use a trick.
        # BETTER APPROACH FOR DEMO:
        # Since we enabled User Workload Monitoring, metrics are in the platform Prometheus.
        # But accessing platform prometheus from user grafana requires auth.
        # Alternative: Scrape vLLM directly from Grafana? No, better to use OCP stack.
        # Let's use the standard "forward via sidecar" or just use the OCP Console?
        # WAIT: The design decision was "User Managed Grafana".
        # Simplest way to get data into User Grafana from OCP Prometheus:
        # 1. Create a ServiceAccount "grafana-sa"
        # 2. Give it `cluster-monitoring-view`
        # 3. Use that token in the datasource.
        # We will configure this datasource to expect a BEARER_TOKEN injected via env var or file, 
        # but for the static configmap, we can't easily put the dynamic token.
        #
        # ALTERNATIVE: Since vLLM is just a pod, we can point Grafana's Prometheus datasource
        # to a SIDE-CAR Prometheus if we wanted isolation, OR just rely on OCP.
        #
        # Let's try the "Service Account Token" approach.
        # The datasource will need `httpHeaderName1: Authorization` and `secureJsonData.httpHeaderValue1: Bearer ...`
        # which is hard in GitOps.
        #
        # SIMPLIFICATION for Step 07:
        # We will deploy a standalone Prometheus *alongside* Grafana for this step to scrape vLLM directly.
        # It's much easier to demo "Grafana + Prometheus" locally in the namespace than dealing with OCP Auth in a demo script.
        # AND it ensures we have high-res data (1s scrape) for the stress test.
        #
        # RE-WRITING DATASOURCE to point to LOCAL prometheus (which we will add).
        url: http://prometheus:9090

