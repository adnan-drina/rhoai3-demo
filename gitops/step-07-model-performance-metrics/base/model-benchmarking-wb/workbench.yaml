# =============================================================================
# Model Benchmarking Workbench - RHOAI 3.0 GitOps Pattern
# =============================================================================
# Interactive Jupyter workbench for running and analyzing GuideLLM benchmarks.
#
# Features:
#   - Run GuideLLM benchmarks directly from notebook
#   - Analyze benchmark results stored in PVC
#   - Visualize TTFT, TPOT, throughput metrics
#   - Compare INT4 vs BF16 model performance
#
# This workbench uses the RHOAI 3.0 controller-managed authentication pattern:
#   - Annotation: notebooks.opendatahub.io/inject-auth: "true"
#   - Controller injects: kube-rbac-proxy sidecar for authentication
# =============================================================================

# -----------------------------------------------------------------------------
# ServiceAccount: Workbench Identity
# -----------------------------------------------------------------------------
apiVersion: v1
kind: ServiceAccount
metadata:
  name: model-benchmarking-wb
  namespace: private-ai
  labels:
    app: model-benchmarking-wb
    opendatahub.io/dashboard: "true"
  annotations:
    argocd.argoproj.io/sync-wave: "5"
---
# -----------------------------------------------------------------------------
# PVC: Workbench Storage
# -----------------------------------------------------------------------------
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: model-benchmarking-wb-storage
  namespace: private-ai
  labels:
    app: model-benchmarking-wb
    opendatahub.io/dashboard: "true"
  annotations:
    argocd.argoproj.io/sync-wave: "5"
    openshift.io/display-name: model-benchmarking-wb-storage
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
  storageClassName: gp3-csi
---
# -----------------------------------------------------------------------------
# Notebook: Model Benchmarking Workbench
# -----------------------------------------------------------------------------
apiVersion: kubeflow.org/v1
kind: Notebook
metadata:
  name: model-benchmarking-wb
  namespace: private-ai
  labels:
    app: model-benchmarking-wb
    opendatahub.io/dashboard: "true"
    opendatahub.io/odh-managed: "true"
  annotations:
    argocd.argoproj.io/sync-wave: "10"
    # RHOAI 3.0: Use inject-auth for kube-rbac-proxy sidecar
    notebooks.opendatahub.io/inject-auth: "true"
    # Hardware Profile: CPU-only for analysis work
    opendatahub.io/hardware-profile-name: cpu-small
    opendatahub.io/hardware-profile-namespace: redhat-ods-applications
    # UI Metadata
    opendatahub.io/image-display-name: "Jupyter | Data Science | CPU | Python 3.12"
    opendatahub.io/username: ai-admin
    notebooks.opendatahub.io/last-image-selection: s2i-generic-data-science-notebook:2025.2
    notebooks.opendatahub.io/last-image-version-git-commit-selection: d3137ca
    notebooks.opendatahub.io/last-size-selection: Small
    openshift.io/display-name: "Model Benchmarking"
    openshift.io/description: "Run and analyze GuideLLM benchmarks - TTFT, TPOT, throughput analysis"
    # Idle Culling: Managed by ODH Notebook Controller (platform default: 1 hour)
    # The controller monitors last-activity and stops idle workbenches automatically
spec:
  template:
    spec:
      enableServiceLinks: false
      serviceAccountName: model-benchmarking-wb
      affinity: {}
      # =========================================================================
      # Git-Sync Init Container: Pre-load benchmarking notebook from Git
      # =========================================================================
      initContainers:
        - name: git-sync
          image: alpine/git:latest
          command:
            - /bin/sh
            - -c
            - |
              set -e
              REPO_URL="https://github.com/adnan-drina/rhoai3-demo.git"
              WORKSPACE="/opt/app-root/src"
              NOTEBOOKS_SRC="gitops/step-07-model-performance-metrics/base/model-benchmarking-wb"
              
              echo "╔══════════════════════════════════════════════════════════════╗"
              echo "║  Git-Sync: Loading Model Benchmarking Notebook               ║"
              echo "╚══════════════════════════════════════════════════════════════╝"
              
              if [ ! -f "$WORKSPACE/Model-Benchmarking.ipynb" ]; then
                echo "→ Cloning repository..."
                git clone --depth 1 --single-branch "$REPO_URL" /tmp/repo
                
                echo "→ Copying notebooks to workspace..."
                cp /tmp/repo/$NOTEBOOKS_SRC/*.ipynb "$WORKSPACE/" 2>/dev/null || true
                
                rm -rf /tmp/repo
                echo "✓ Notebooks synced successfully!"
              else
                echo "✓ Notebooks already exist - skipping sync"
              fi
              
              ls -la "$WORKSPACE/"
          volumeMounts:
            - name: model-benchmarking-wb-storage
              mountPath: /opt/app-root/src
          resources:
            limits:
              cpu: 100m
              memory: 128Mi
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop: ["ALL"]
      # =========================================================================
      # Container: Jupyter Notebook
      # =========================================================================
      containers:
        - name: model-benchmarking-wb
          image: image-registry.openshift-image-registry.svc:5000/redhat-ods-applications/s2i-generic-data-science-notebook:2025.2
          imagePullPolicy: Always
          workingDir: /opt/app-root/src
          env:
            - name: NOTEBOOK_ARGS
              value: |-
                --ServerApp.port=8888
                --ServerApp.token=''
                --ServerApp.password=''
                --ServerApp.base_url=/notebook/private-ai/model-benchmarking-wb
                --ServerApp.quit_button=False
            - name: JUPYTER_IMAGE
              value: image-registry.openshift-image-registry.svc:5000/redhat-ods-applications/s2i-generic-data-science-notebook:2025.2
            - name: PIP_CERT
              value: /etc/pki/tls/custom-certs/ca-bundle.crt
            - name: REQUESTS_CA_BUNDLE
              value: /etc/pki/tls/custom-certs/ca-bundle.crt
            - name: SSL_CERT_FILE
              value: /etc/pki/tls/custom-certs/ca-bundle.crt
            - name: PIPELINES_SSL_SA_CERTS
              value: /etc/pki/tls/custom-certs/ca-bundle.crt
            - name: GIT_SSL_CAINFO
              value: /etc/pki/tls/custom-certs/ca-bundle.crt
            # GuideLLM configuration
            - name: GUIDELLM_RESULTS_DIR
              value: /results
          ports:
            - containerPort: 8888
              name: notebook-port
              protocol: TCP
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: "2"
              memory: 4Gi
          livenessProbe:
            httpGet:
              path: /notebook/private-ai/model-benchmarking-wb/api
              port: notebook-port
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /notebook/private-ai/model-benchmarking-wb/api
              port: notebook-port
              scheme: HTTP
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 1
            successThreshold: 1
            failureThreshold: 3
          volumeMounts:
            - name: model-benchmarking-wb-storage
              mountPath: /opt/app-root/src
            - name: guidellm-results
              mountPath: /results
              readOnly: true
            - name: pipeline-results
              mountPath: /pipeline-results
              readOnly: true
            - name: shm
              mountPath: /dev/shm
            - name: trusted-ca
              mountPath: /etc/pki/tls/custom-certs/ca-bundle.crt
              subPath: ca-bundle.crt
              readOnly: true
      volumes:
        - name: model-benchmarking-wb-storage
          persistentVolumeClaim:
            claimName: model-benchmarking-wb-storage
        - name: guidellm-results
          persistentVolumeClaim:
            claimName: guidellm-results
        - name: pipeline-results
          persistentVolumeClaim:
            claimName: guidellm-pipeline-results
        - name: shm
          emptyDir:
            medium: Memory
        - name: trusted-ca
          configMap:
            name: workbench-trusted-ca-bundle
            optional: true
            items:
              - key: ca-bundle.crt
                path: ca-bundle.crt
---
# =============================================================================
# RBAC: Model Benchmarking Permissions
# =============================================================================
# Permissions to run benchmarks and view results
# =============================================================================
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: model-benchmarking-controller
  namespace: private-ai
  labels:
    app: model-benchmarking-wb
  annotations:
    argocd.argoproj.io/sync-wave: "5"
rules:
  # Create and monitor benchmark Jobs
  - apiGroups: ["batch"]
    resources: ["jobs", "cronjobs"]
    verbs: ["get", "list", "watch", "create", "delete"]
  # View PipelineRuns
  - apiGroups: ["tekton.dev"]
    resources: ["pipelineruns", "taskruns"]
    verbs: ["get", "list", "watch", "create"]
  # View Pods for log access
  - apiGroups: [""]
    resources: ["pods", "pods/log"]
    verbs: ["get", "list", "watch"]
  # View InferenceServices (to get model endpoints)
  - apiGroups: ["serving.kserve.io"]
    resources: ["inferenceservices"]
    verbs: ["get", "list", "watch"]
  # View ConfigMaps (for GuideLLM config)
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: model-benchmarking-controller
  namespace: private-ai
  labels:
    app: model-benchmarking-wb
  annotations:
    argocd.argoproj.io/sync-wave: "5"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: model-benchmarking-controller
subjects:
  - kind: ServiceAccount
    name: model-benchmarking-wb
    namespace: private-ai

