{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üéØ Model Performance Benchmarking\n",
        "\n",
        "**The ROI of Quantization** - Run and analyze GuideLLM benchmarks to measure vLLM inference performance.\n",
        "\n",
        "## What This Notebook Does\n",
        "\n",
        "1. **Run Benchmarks** - Execute GuideLLM tests against deployed models\n",
        "2. **Analyze Results** - Parse benchmark JSON files from PVC\n",
        "3. **Visualize Metrics** - Plot TTFT, TPOT, throughput curves\n",
        "4. **Compare Models** - INT4 vs BF16 efficiency analysis\n",
        "\n",
        "---\n",
        "\n",
        "## Volume Mounts\n",
        "\n",
        "| Path | Source | Contents |\n",
        "|------|--------|----------|\n",
        "| `/results` | `guidellm-results` PVC | CronJob benchmark results |\n",
        "| `/pipeline-results` | `guidellm-pipeline-results` PVC | Tekton Pipeline results |\n",
        "| `/opt/app-root/src` | Workbench storage | Your notebooks and analysis |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q pandas matplotlib plotly requests tabulate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import glob\n",
        "import subprocess\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Configure paths\n",
        "CRONJOB_RESULTS = Path(\"/results\")\n",
        "PIPELINE_RESULTS = Path(\"/pipeline-results\")\n",
        "\n",
        "# Model endpoints\n",
        "MODELS = {\n",
        "    \"mistral-3-int4\": \"http://mistral-3-int4-predictor.private-ai.svc.cluster.local:8080/v1\",\n",
        "    \"mistral-3-bf16\": \"http://mistral-3-bf16-predictor.private-ai.svc.cluster.local:8080/v1\",\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Environment ready\")\n",
        "print(f\"\\nüìÅ CronJob results: {CRONJOB_RESULTS}\")\n",
        "print(f\"üìÅ Pipeline results: {PIPELINE_RESULTS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Check Available Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List InferenceServices\n",
        "!oc get inferenceservice -n private-ai -o custom-columns=NAME:.metadata.name,READY:.status.conditions[0].status,URL:.status.url\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick health check\n",
        "import requests\n",
        "\n",
        "def check_model_health(name, url):\n",
        "    try:\n",
        "        resp = requests.get(f\"{url}/models\", timeout=5)\n",
        "        if resp.status_code == 200:\n",
        "            models = resp.json().get(\"data\", [])\n",
        "            return f\"‚úÖ {name}: {len(models)} model(s) loaded\"\n",
        "        return f\"‚ö†Ô∏è {name}: HTTP {resp.status_code}\"\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå {name}: {str(e)[:50]}\"\n",
        "\n",
        "print(\"\\nüîç Model Health Check\\n\" + \"=\"*50)\n",
        "for name, url in MODELS.items():\n",
        "    print(check_model_health(name, url))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure benchmark parameters\n",
        "BENCHMARK_CONFIG = {\n",
        "    \"model\": \"mistral-3-int4\",           # Model to benchmark\n",
        "    \"profile\": \"constant\",                # constant, poisson, sweep\n",
        "    \"rate_type\": \"concurrent\",            # concurrent, constant\n",
        "    \"rates\": [1, 3, 5, 8, 10],             # Concurrency levels to test\n",
        "    \"max_seconds\": 30,                     # Duration per rate level\n",
        "    \"max_requests\": 50,                    # Max requests per rate level\n",
        "    \"prompt_tokens\": 256,                  # Synthetic input tokens\n",
        "    \"output_tokens\": 256,                  # Synthetic output tokens\n",
        "}\n",
        "\n",
        "print(\"üìã Benchmark Configuration:\")\n",
        "for k, v in BENCHMARK_CONFIG.items():\n",
        "    print(f\"   {k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Trigger the daily CronJob manually (simplest approach)\n",
        "!oc create job --from=cronjob/guidellm-daily benchmark-notebook-$(date +%H%M%S) -n private-ai\n",
        "\n",
        "print(\"\\nüìä Monitor with: oc logs -f job/benchmark-notebook-... -n private-ai\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Analyze Benchmark Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List available result files\n",
        "def list_results(results_dir: Path) -> list:\n",
        "    \"\"\"List all JSON result files in a directory.\"\"\"\n",
        "    if not results_dir.exists():\n",
        "        return []\n",
        "    files = sorted(results_dir.glob(\"**/*.json\"), key=os.path.getmtime, reverse=True)\n",
        "    return files[:20]  # Return most recent 20\n",
        "\n",
        "print(\"üìÅ CronJob Results (most recent):\")\n",
        "for f in list_results(CRONJOB_RESULTS)[:5]:\n",
        "    size = f.stat().st_size / 1024\n",
        "    mtime = datetime.fromtimestamp(f.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M\")\n",
        "    print(f\"   {f.name:<40} {size:>8.1f} KB  {mtime}\")\n",
        "\n",
        "print(\"\\nüìÅ Pipeline Results (most recent):\")\n",
        "for f in list_results(PIPELINE_RESULTS)[:5]:\n",
        "    size = f.stat().st_size / 1024\n",
        "    mtime = datetime.fromtimestamp(f.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M\")\n",
        "    print(f\"   {f.name:<40} {size:>8.1f} KB  {mtime}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_guidellm_results(filepath: Path) -> dict:\n",
        "    \"\"\"Parse GuideLLM benchmark results JSON.\"\"\"\n",
        "    with open(filepath) as f:\n",
        "        data = json.load(f)\n",
        "    \n",
        "    benchmarks = data.get(\"benchmarks\", [])\n",
        "    results = []\n",
        "    \n",
        "    for bench in benchmarks:\n",
        "        metrics = bench.get(\"metrics\", {})\n",
        "        results.append({\n",
        "            \"rate\": bench.get(\"rate\", 0),\n",
        "            \"rate_type\": bench.get(\"rate_type\", \"unknown\"),\n",
        "            \"completed_requests\": metrics.get(\"request_count\", 0),\n",
        "            \"throughput_tok_s\": metrics.get(\"output_token_throughput\", {}).get(\"mean\", 0),\n",
        "            \"ttft_p50_ms\": metrics.get(\"ttft\", {}).get(\"p50\", 0) * 1000,\n",
        "            \"ttft_p95_ms\": metrics.get(\"ttft\", {}).get(\"p95\", 0) * 1000,\n",
        "            \"tpot_p50_ms\": metrics.get(\"itl\", {}).get(\"p50\", 0) * 1000,\n",
        "            \"tpot_p95_ms\": metrics.get(\"itl\", {}).get(\"p95\", 0) * 1000,\n",
        "        })\n",
        "    \n",
        "    return {\n",
        "        \"model\": data.get(\"model\", \"unknown\"),\n",
        "        \"target\": data.get(\"target\", \"unknown\"),\n",
        "        \"benchmarks\": results\n",
        "    }\n",
        "\n",
        "# Parse and display most recent result\n",
        "recent_files = list_results(CRONJOB_RESULTS)\n",
        "if recent_files:\n",
        "    result = parse_guidellm_results(recent_files[0])\n",
        "    print(f\"\\nüìä Latest Result: {recent_files[0].name}\")\n",
        "    print(f\"   Model: {result['model']}\")\n",
        "    print(f\"   Benchmarks: {len(result['benchmarks'])} rate levels\")\n",
        "    \n",
        "    # Show as table\n",
        "    df = pd.DataFrame(result[\"benchmarks\"])\n",
        "    print(\"\\n\" + df.to_string(index=False))\n",
        "else:\n",
        "    print(\"No results found. Run a benchmark first!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualize Performance Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_latency_vs_concurrency(results: list, title: str = \"Latency vs Concurrency\"):\n",
        "    \"\"\"Plot TTFT and TPOT against concurrency levels.\"\"\"\n",
        "    df = pd.DataFrame(results)\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # TTFT plot\n",
        "    axes[0].plot(df[\"rate\"], df[\"ttft_p50_ms\"], \"b-o\", label=\"P50\")\n",
        "    axes[0].plot(df[\"rate\"], df[\"ttft_p95_ms\"], \"r-s\", label=\"P95\")\n",
        "    axes[0].axhline(y=1000, color=\"orange\", linestyle=\"--\", label=\"SLA (1s)\")\n",
        "    axes[0].axhline(y=2000, color=\"red\", linestyle=\"--\", label=\"Breaking (2s)\")\n",
        "    axes[0].set_xlabel(\"Concurrent Users\")\n",
        "    axes[0].set_ylabel(\"TTFT (ms)\")\n",
        "    axes[0].set_title(\"Time to First Token\")\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # TPOT plot\n",
        "    axes[1].plot(df[\"rate\"], df[\"tpot_p50_ms\"], \"b-o\", label=\"P50\")\n",
        "    axes[1].plot(df[\"rate\"], df[\"tpot_p95_ms\"], \"r-s\", label=\"P95\")\n",
        "    axes[1].axhline(y=100, color=\"orange\", linestyle=\"--\", label=\"SLA (100ms)\")\n",
        "    axes[1].set_xlabel(\"Concurrent Users\")\n",
        "    axes[1].set_ylabel(\"TPOT (ms)\")\n",
        "    axes[1].set_title(\"Time per Output Token\")\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.suptitle(title, fontsize=14, fontweight=\"bold\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot if results exist\n",
        "if recent_files:\n",
        "    result = parse_guidellm_results(recent_files[0])\n",
        "    if result[\"benchmarks\"]:\n",
        "        plot_latency_vs_concurrency(result[\"benchmarks\"], f\"Model: {result['model']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_throughput_curve(results: list, title: str = \"Throughput vs Concurrency\"):\n",
        "    \"\"\"Plot throughput against concurrency levels.\"\"\"\n",
        "    df = pd.DataFrame(results)\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=(10, 5))\n",
        "    \n",
        "    ax.plot(df[\"rate\"], df[\"throughput_tok_s\"], \"g-o\", linewidth=2, markersize=8)\n",
        "    ax.fill_between(df[\"rate\"], df[\"throughput_tok_s\"], alpha=0.3, color=\"green\")\n",
        "    \n",
        "    ax.set_xlabel(\"Concurrent Users\", fontsize=12)\n",
        "    ax.set_ylabel(\"Throughput (tokens/sec)\", fontsize=12)\n",
        "    ax.set_title(title, fontsize=14, fontweight=\"bold\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Annotate max throughput\n",
        "    max_idx = df[\"throughput_tok_s\"].idxmax()\n",
        "    max_rate = df.loc[max_idx, \"rate\"]\n",
        "    max_throughput = df.loc[max_idx, \"throughput_tok_s\"]\n",
        "    ax.annotate(f\"Peak: {max_throughput:.0f} tok/s @ {max_rate} users\",\n",
        "                xy=(max_rate, max_throughput),\n",
        "                xytext=(max_rate + 2, max_throughput * 0.9),\n",
        "                fontsize=10,\n",
        "                arrowprops=dict(arrowstyle=\"->\", color=\"black\"))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot if results exist\n",
        "if recent_files:\n",
        "    result = parse_guidellm_results(recent_files[0])\n",
        "    if result[\"benchmarks\"]:\n",
        "        plot_throughput_curve(result[\"benchmarks\"], f\"Throughput: {result['model']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. ROI Analysis: INT4 vs BF16\n",
        "\n",
        "Calculate the economics of quantization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hardware costs (AWS on-demand pricing)\n",
        "COSTS = {\n",
        "    \"int4\": {\"gpus\": 1, \"instance\": \"g6.4xlarge\", \"cost_hr\": 0.85},\n",
        "    \"bf16\": {\"gpus\": 4, \"instance\": \"g6.12xlarge\", \"cost_hr\": 3.40},\n",
        "}\n",
        "\n",
        "# Find INT4 and BF16 results\n",
        "def find_model_results(model_name: str, results_dir: Path) -> Path:\n",
        "    pattern = f\"*{model_name}*.json\"\n",
        "    files = sorted(results_dir.glob(pattern), key=os.path.getmtime, reverse=True)\n",
        "    return files[0] if files else None\n",
        "\n",
        "int4_file = find_model_results(\"int4\", CRONJOB_RESULTS)\n",
        "bf16_file = find_model_results(\"bf16\", CRONJOB_RESULTS)\n",
        "\n",
        "print(\"üîç Found result files:\")\n",
        "print(f\"   INT4: {int4_file.name if int4_file else 'Not found'}\")\n",
        "print(f\"   BF16: {bf16_file.name if bf16_file else 'Not found'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_roi(int4_results: dict, bf16_results: dict):\n",
        "    \"\"\"Calculate ROI metrics for INT4 vs BF16.\"\"\"\n",
        "    \n",
        "    int4_df = pd.DataFrame(int4_results[\"benchmarks\"])\n",
        "    bf16_df = pd.DataFrame(bf16_results[\"benchmarks\"])\n",
        "    \n",
        "    int4_peak = int4_df[\"throughput_tok_s\"].max()\n",
        "    bf16_peak = bf16_df[\"throughput_tok_s\"].max()\n",
        "    \n",
        "    int4_efficiency = int4_peak / COSTS[\"int4\"][\"cost_hr\"]\n",
        "    bf16_efficiency = bf16_peak / COSTS[\"bf16\"][\"cost_hr\"]\n",
        "    \n",
        "    print(\"\\nüí∞ ROI Analysis: The Economics of Precision\\n\" + \"=\"*55)\n",
        "    print(f\"\\n{'Metric':<30} {'INT4 (1-GPU)':<15} {'BF16 (4-GPU)':<15}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Hardware Cost':<30} ${COSTS['int4']['cost_hr']:.2f}/hr{'':<9} ${COSTS['bf16']['cost_hr']:.2f}/hr\")\n",
        "    print(f\"{'Peak Throughput':<30} {int4_peak:.0f} tok/s{'':<7} {bf16_peak:.0f} tok/s\")\n",
        "    print(f\"{'Efficiency (tok/s per $)':<30} {int4_efficiency:.0f}{'':<14} {bf16_efficiency:.0f}\")\n",
        "    print(f\"{'Cost Ratio':<30} 1x{'':<15} {COSTS['bf16']['cost_hr']/COSTS['int4']['cost_hr']:.1f}x\")\n",
        "    \n",
        "    # 4x INT4 vs 1x BF16 comparison\n",
        "    four_int4_throughput = int4_peak * 4\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*55)\n",
        "    print(\"\\nüéØ Key Insight: 4x INT4 vs 1x BF16 (Same Cost)\")\n",
        "    print(f\"   4x INT4 Throughput: {four_int4_throughput:.0f} tok/s\")\n",
        "    print(f\"   1x BF16 Throughput: {bf16_peak:.0f} tok/s\")\n",
        "    if four_int4_throughput > bf16_peak:\n",
        "        print(f\"   Advantage: INT4 delivers {(four_int4_throughput/bf16_peak - 1)*100:.0f}% more throughput!\")\n",
        "    else:\n",
        "        print(f\"   Note: BF16 delivers {(bf16_peak/four_int4_throughput - 1)*100:.0f}% more throughput\")\n",
        "\n",
        "if int4_file and bf16_file:\n",
        "    int4_results = parse_guidellm_results(int4_file)\n",
        "    bf16_results = parse_guidellm_results(bf16_file)\n",
        "    calculate_roi(int4_results, bf16_results)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Need both INT4 and BF16 results for ROI analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üìö References\n",
        "\n",
        "- [GuideLLM Documentation](https://github.com/neuralmagic/guidellm)\n",
        "- [vLLM Production Metrics](https://docs.vllm.ai/en/latest/serving/metrics.html)\n",
        "- [Red Hat AI Benchmarking Guide](https://developers.redhat.com/articles/2025/06/20/guidellm-evaluate-llm-deployments-real-world-inference)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
