# LLMInferenceService: Distributed Inference with llm-d
# 
# This CR deploys a model using llm-d's distributed inference capability,
# enabling horizontal scaling across multiple GPU nodes.
#
# Configuration aligned with Red Hat reference implementation:
# https://github.com/rh-aiservices-bu/rhaoi3-llm-d
#
# ═══════════════════════════════════════════════════════════════════════════════
# DESIGN DECISION: Replicas Mode vs Tensor Parallelism
# ═══════════════════════════════════════════════════════════════════════════════
#
# This demo uses REPLICAS MODE (tensor:1, replicas:2) — NOT Tensor Parallelism.
#
# | Mode              | Configuration           | What It Creates    | Use Case                  |
# |-------------------|-------------------------|--------------------|-----------------------------|
# | Replicas (current)| tensor:1, replicas:2    | Deployment         | KV-cache routing demo       |
# | Tensor Parallel   | tensor:2, replicas:1    | LeaderWorkerSet    | Model sharding (70B+ models)|
#
# Why Replicas Mode for this demo:
#   1. Continues Step 07 benchmark story — compares routing efficiency vs round-robin
#   2. Shows llm-d's unique value — ~63% faster P95 TTFT, ~90% cache hit rate
#   3. Model fits on 1 GPU — Mistral INT4 (~12GB) doesn't need sharding
#   4. Matches reference repo — comparable benchmark results
#
# Expected improvements (vs vLLM round-robin):
#   - P50 TTFT: ~25% faster
#   - P95 TTFT: ~63% faster
#   - Cache speedup: 2.1x better
#
# ═══════════════════════════════════════════════════════════════════════════════
#
# Prerequisites:
#   - 2× g6.4xlarge nodes with available GPU capacity
#   - Gateway API configured (optional, OpenShift Route also works)
#
# Dashboard Visibility:
#   LLMInferenceService (v1alpha1) does NOT appear in the RHOAI Dashboard.
#   This is expected behavior per RHOAI 3.0 - llm-d is managed via CLI only.
#   Use 'oc get llminferenceservice' to monitor status.
#
# ⚠️  DEMO-ONLY: Authentication is disabled for demo simplicity.
#     For production, implement Red Hat Connectivity Link (RHCL) auth per RHOAI docs.
#
# Ref: https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0/html-single/deploying_models/index

apiVersion: serving.kserve.io/v1alpha1
kind: LLMInferenceService
metadata:
  name: mistral-3-distributed
  namespace: private-ai
  labels:
    # Route to the reserved llm-d LocalQueue (created in Step 03)
    # This ensures llm-d gets dedicated GPU capacity from rhoai-llmd-queue
    # even when the main vLLM queue is saturated
    kueue.x-k8s.io/queue-name: llmd
    # Kubernetes recommended labels
    app.kubernetes.io/name: mistral-3-distributed
    app.kubernetes.io/component: inference
    app.kubernetes.io/part-of: llm-serving
    app.kubernetes.io/managed-by: argocd
    demo.rhoai.io/step: "08"
  annotations:
    argocd.argoproj.io/sync-wave: "1"
    # RHOAI annotations (matching reference repo)
    opendatahub.io/hardware-profile-name: nvidia-l4-1gpu
    opendatahub.io/hardware-profile-namespace: redhat-ods-applications
    opendatahub.io/model-type: generative
    # Disable authentication for demo simplicity (per reference repo)
    security.opendatahub.io/enable-auth: "false"
    # Display metadata
    openshift.io/display-name: "Mistral-3 Distributed (llm-d)"
    openshift.io/description: "2 replicas with intelligent routing - demonstrates KV cache awareness and tail latency improvement"
spec:
  # ═══════════════════════════════════════════════════════════════════════════
  # Model Configuration
  # ═══════════════════════════════════════════════════════════════════════════
  model:
    name: mistral-3-distributed
    # Same INT4 ModelCar image as Step 05 baseline
    uri: oci://registry.redhat.io/rhelai1/modelcar-mistral-small-24b-instruct-2501-quantized-w4a16:1.5
  
  # ═══════════════════════════════════════════════════════════════════════════
  # Parallelism Configuration (Routing Demo)
  # ═══════════════════════════════════════════════════════════════════════════
  # No tensor parallelism - each replica runs on a single GPU
  # This matches the reference repo pattern for demonstrating routing benefits
  parallelism:
    tensor: 1
  
  # ═══════════════════════════════════════════════════════════════════════════
  # Replica Configuration (Routing Demo)
  # ═══════════════════════════════════════════════════════════════════════════
  # 2 replicas = 2 independent model instances
  # llm-d router can now demonstrate intelligent routing:
  #   - Routes requests to replica with cached KV prefix
  #   - Shows 90%+ cache hit rate vs ~25% with round-robin
  #   - Significantly lower P95/P99 tail latency
  replicas: 2
  
  # ═══════════════════════════════════════════════════════════════════════════
  # Pod Template (GPU Scheduling)
  # ═══════════════════════════════════════════════════════════════════════════
  # NOTE: The llm-d controller injects a container named "main". To configure
  # GPU resources for that container, we must use spec.template.containers with
  # name: "main" (not a custom name like "vllm").
  template:
    # Target g6.4xlarge nodes (single L4 GPU each)
    nodeSelector:
      node.kubernetes.io/instance-type: g6.4xlarge
    
    # Tolerate GPU node taint
    tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
    
    # Container resources - must match controller's container name "main"
    containers:
      - name: main
        resources:
          limits:
            cpu: "16"
            memory: 60Gi
            nvidia.com/gpu: "1"
          requests:
            cpu: "8"
            memory: 32Gi
            nvidia.com/gpu: "1"
        # Override liveness probe for longer model loading time
        # Reference: rh-aiservices-bu/rhaoi3-llm-d uses 120s + 30s × 5 = 270s for 0.6B model
        # Our 24B INT4 model needs ~4-5 min for: model load + torch.compile + CUDA graphs
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
            scheme: HTTPS
          initialDelaySeconds: 300  # Wait 5 min before first check (24B >> 0.6B)
          periodSeconds: 30         # Check every 30s (same as reference)
          timeoutSeconds: 30        # Allow 30s for health check response
          failureThreshold: 10      # 10 failures × 30s = 5 more minutes
  
  # ═══════════════════════════════════════════════════════════════════════════
  # Router Configuration
  # ═══════════════════════════════════════════════════════════════════════════
  # NOTE (demo safety):
  # This repo ships Step 08 **without** creating any additional Gateway/LB by default,
  # to avoid accidental `*.apps` DNS disruptions on managed clusters.
  #
  # If you want an externally reachable HTTPRoute for llm-d, apply the overlay:
  #   gitops/step-08-llm-d/overlays/external-gateway/
  #
  # Ref: https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0/html-single/deploying_models/index
  router:
    route: {}
    scheduler: {}

