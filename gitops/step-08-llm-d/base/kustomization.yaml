apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

# Step 08: Distributed Inference with llm-d
# Demonstrates horizontal scaling of LLM inference across multiple GPU nodes
# PLUS: SLO-driven autoscaling with KEDA
#
# Architecture:
#   ┌─────────────────────────────────────────────────────────────────┐
#   │                    private-ai namespace                         │
#   ├─────────────────────────────────────────────────────────────────┤
#   │                                                                 │
#   │   ┌─────────────────┐     ┌─────────────────────────────────┐  │
#   │   │  llm-d Router   │◀────│        Gateway API              │  │
#   │   │  (scheduler)    │     │  (data-science-gateway)         │  │
#   │   └────────┬────────┘     └─────────────────────────────────┘  │
#   │            │                                                    │
#   │            ├───────────────────┐                               │
#   │            │                   │                               │
#   │            ▼                   ▼                               │
#   │   ┌────────────────┐  ┌────────────────┐                      │
#   │   │  Worker Pod 0  │  │  Worker Pod 1  │                      │
#   │   │  g6.4xlarge    │  │  g6.4xlarge    │                      │
#   │   │  1× NVIDIA L4  │  │  1× NVIDIA L4  │                      │
#   │   └────────────────┘  └────────────────┘                      │
#   │            ▲                   ▲                               │
#   │            └───────────────────┴────────────────────────────┐  │
#   │                                                             │  │
#   │   ┌─────────────────────────────────────────────────────────┘  │
#   │   │  KEDA ScaledObject                                         │
#   │   │  - Scales on ITL P50 > 75ms                                │
#   │   │  - Scales on Queue Depth > 5                               │
#   │   └────────────────────────────────────────────────────────────│
#   │                                                                 │
#   └─────────────────────────────────────────────────────────────────┘
#
# Prerequisites:
#   - LeaderWorkerSet operator installed (Step 01)
#   - Red Hat Connectivity Link (RHCL) operator installed (Step 01)
#   - Gateway API configured (RHOAI 3.0 default)
#   - 2× g6.4xlarge GPU nodes available
#
# Ref: https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0/html-single/deploying_models/index
# Ref: https://developers.redhat.com/articles/2025/11/26/autoscaling-vllm-openshift-ai-model-serving

resources:
  # ═══════════════════════════════════════════════════════════════════════════
  # Custom Metrics Autoscaler (CMA) Operator (sync-wave: -5 to -1)
  # Provides KEDA-based SLO-driven autoscaling
  # Based on: https://github.com/redhat-cop/gitops-catalog/tree/main/openshift-custom-metrics-autoscaler
  # ═══════════════════════════════════════════════════════════════════════════
  - cma-operator/
  
  # ═══════════════════════════════════════════════════════════════════════════
  # Red Hat Connectivity Link (RHCL) instance (sync-wave: 0)
  # Required for llm-d Gateway integration and AuthPolicy validation
  # ═══════════════════════════════════════════════════════════════════════════
  - rhcl/
  
  # ═══════════════════════════════════════════════════════════════════════════
  # Distributed Inference Service (sync-wave: 1)
  # ═══════════════════════════════════════════════════════════════════════════
  - llm-d/

  # ═══════════════════════════════════════════════════════════════════════════
  # KEDA Autoscaling (sync-wave: 3-5)
  # SLO-driven autoscaling based on ITL and queue depth
  # ═══════════════════════════════════════════════════════════════════════════
  - keda-autoscaling/

  # ═══════════════════════════════════════════════════════════════════════════
  # Observability (sync-wave: 10-12)
  # ServiceMonitors + GrafanaDashboard for llm-d story and comparisons
  # (Assumes Step 07 Grafana instance exists in private-ai)
  # ═══════════════════════════════════════════════════════════════════════════
  - observability/
  
  # ═══════════════════════════════════════════════════════════════════════════
  # Benchmark Resources (sync-wave: 5)
  # Dedicated GuideLLM benchmark for distributed endpoint
  # ═══════════════════════════════════════════════════════════════════════════
  - benchmark/

