# ScaledObject for llm-d Autoscaling
#
# ═══════════════════════════════════════════════════════════════════════════════
# IMPORTANT: Scaling Target depends on parallelism mode
# ═══════════════════════════════════════════════════════════════════════════════
#
# Current demo uses: tensor:1, replicas:2 → creates Deployment
# If using: tensor:2, replicas:1 → creates LeaderWorkerSet (LWS)
#
# For LWS mode, target would be:
#   scaleTargetRef:
#     apiVersion: leaderworkerset.x-k8s.io/v1
#     kind: LeaderWorkerSet
#     name: mistral-3-distributed
#
# Scaling 1 LWS replica adds N pods (N = tensor size = shard count)
# This maintains Tensor Parallel (TP) integrity.
#
# ═══════════════════════════════════════════════════════════════════════════════
#
# SLO-driven autoscaling based on:
#   1. Queue Depth - Primary: Users waiting in vLLM queue (KV Cache pressure)
#   2. Inter-Token Latency (ITL) - Secondary: Decode phase quality
#
# Ref: https://developers.redhat.com/articles/2025/11/26/autoscaling-vllm-openshift-ai-model-serving
# Ref: https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/nodes/deploying-the-custom-metrics-autoscaler
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: mistral-3-distributed-autoscaler
  namespace: private-ai
  labels:
    app.kubernetes.io/name: mistral-3-distributed
    app.kubernetes.io/component: autoscaling
    demo.rhoai.io/step: "08"
  annotations:
    argocd.argoproj.io/sync-wave: "5"
    argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true
    openshift.io/display-name: "llm-d SLO-Driven Autoscaler"
    openshift.io/description: "Scales llm-d based on ITL and queue depth metrics"
spec:
  # Target the llm-d workload Deployment
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mistral-3-distributed-kserve
  
  # Scaling bounds
  # With 600s cooldown/stabilization, KEDA waits long enough for model loading
  # (24B INT4 takes ~2 min) before making scale-down decisions.
  minReplicaCount: 1    # Allow full scale-down for autoscaling demo
  maxReplicaCount: 3    # Limited by GPU nodes (3× g6.4xlarge, 1 used by INT4 vLLM)
  
  # ═══════════════════════════════════════════════════════════════════════════
  # Polling and cooldown
  # ═══════════════════════════════════════════════════════════════════════════
  # IMPORTANT: For LLM workloads, cooldownPeriod must be LONGER than model
  # loading time. A 24B INT4 model takes 3-5+ minutes to load into GPU memory.
  # If cooldownPeriod is too short, KEDA will kill pods before they're ready.
  pollingInterval: 15     # Check metrics every 15s
  cooldownPeriod: 600     # Wait 10min before scaling down (model loading is slow!)
  
  # Scaling behavior
  advanced:
    horizontalPodAutoscalerConfig:
      behavior:
        scaleDown:
          # Long stabilization prevents thrashing during model loading
          stabilizationWindowSeconds: 600  # 10min stabilization for LLM
          policies:
            - type: Pods
              value: 1
              periodSeconds: 120  # Scale down 1 pod per 2 minutes max
        scaleUp:
          stabilizationWindowSeconds: 60   # 1min to scale up
          policies:
            - type: Pods
              value: 1
              periodSeconds: 30
  
  triggers:
    # ═══════════════════════════════════════════════════════════════════════════
    # PRIMARY TRIGGER: Queue Depth (vllm:num_requests_waiting)
    # ═══════════════════════════════════════════════════════════════════════════
    # This is the most effective metric for llm-d per Red Hat Engineering.
    # It represents users stuck in queue because KV Cache is full.
    # Target: Scale up when average queue length across shards > 5
    - type: prometheus
      metadata:
        # ═══════════════════════════════════════════════════════════════════════
        # Thanos Querier Configuration (per Red Hat Developer Article)
        # ═══════════════════════════════════════════════════════════════════════
        # Ref: https://developers.redhat.com/articles/2025/09/23/how-set-kserve-autoscaling-vllm-keda
        # The 'namespace' field is passed to Thanos port 9092 for tenancy filtering.
        serverAddress: https://thanos-querier.openshift-monitoring.svc.cluster.local:9092
        metricName: vllm_queue_depth
        namespace: private-ai
        # Average queue depth across all model replicas
        query: 'sum(vllm:num_requests_waiting{namespace="private-ai", pod=~"mistral-3-distributed.*"})'
        threshold: "5"  # Scale up when avg queue > 5
        ignoreNullValues: "true"
        unsafeSsl: "true"
        authModes: "bearer"
      authenticationRef:
        name: keda-trigger-auth-prometheus
    
    # ═══════════════════════════════════════════════════════════════════════════
    # SECONDARY TRIGGER: Inter-Token Latency (ITL) - P95
    # ═══════════════════════════════════════════════════════════════════════════
    # ITL measures decode phase latency - the user-perceived "streaming speed".
    # 
    # Calibration based on Step 07 benchmark results:
    #   - P95 TPOT baseline (1 user): 56ms
    #   - P95 TPOT at capacity (8 users): 63ms  
    #   - P95 TPOT at breaking point (15 users): 75ms
    #
    # Threshold: 70ms (before severe degradation, per Step 07 SLA table)
    # Using P95 instead of P50 per Red Hat Developer recommendation:
    #   "Our use of a p50 (median) ITL might not be as effective as a p90 or p95 target"
    #
    # Ref: https://developers.redhat.com/articles/2025/11/26/autoscaling-vllm-openshift-ai-model-serving
    - type: prometheus
      metadata:
        serverAddress: https://thanos-querier.openshift-monitoring.svc.cluster.local:9092
        metricName: vllm_itl_p95_ms
        namespace: private-ai
        # P95 ITL in milliseconds (better signal than P50 per Red Hat article)
        query: |
          histogram_quantile(0.95,
            sum(rate(vllm:time_per_output_token_seconds_bucket{namespace="private-ai", pod=~"mistral-3-distributed.*"}[2m])) by (le)
          ) * 1000
        threshold: "70"  # Scale up when P95 ITL > 70ms (before 75ms breaking point)
        ignoreNullValues: "true"
        unsafeSsl: "true"
        authModes: "bearer"
      authenticationRef:
        name: keda-trigger-auth-prometheus

