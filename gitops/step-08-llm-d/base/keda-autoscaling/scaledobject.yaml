# ScaledObject for llm-d Autoscaling
#
# ═══════════════════════════════════════════════════════════════════════════════
# IMPORTANT: Scaling Target depends on parallelism mode
# ═══════════════════════════════════════════════════════════════════════════════
#
# Current demo uses: tensor:1, replicas:2 → creates Deployment
# If using: tensor:2, replicas:1 → creates LeaderWorkerSet (LWS)
#
# For LWS mode, target would be:
#   scaleTargetRef:
#     apiVersion: leaderworkerset.x-k8s.io/v1
#     kind: LeaderWorkerSet
#     name: mistral-3-distributed
#
# Scaling 1 LWS replica adds N pods (N = tensor size = shard count)
# This maintains Tensor Parallel (TP) integrity.
#
# ═══════════════════════════════════════════════════════════════════════════════
#
# SLO-driven autoscaling based on:
#   1. Queue Depth - Primary: Users waiting in vLLM queue (KV Cache pressure)
#   2. Inter-Token Latency (ITL) - Secondary: Decode phase quality
#
# Ref: https://developers.redhat.com/articles/2025/11/26/autoscaling-vllm-openshift-ai-model-serving
# Ref: https://docs.redhat.com/en/documentation/openshift_container_platform/4.20/html/nodes/deploying-the-custom-metrics-autoscaler
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: mistral-3-distributed-autoscaler
  namespace: private-ai
  labels:
    app.kubernetes.io/name: mistral-3-distributed
    app.kubernetes.io/component: autoscaling
    demo.rhoai.io/step: "08"
  annotations:
    argocd.argoproj.io/sync-wave: "5"
    argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true
    openshift.io/display-name: "llm-d SLO-Driven Autoscaler"
    openshift.io/description: "Scales llm-d based on ITL and queue depth metrics"
spec:
  # Target the llm-d workload Deployment
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mistral-3-distributed-kserve
  
  # Scaling bounds
  # NOTE: minReplicaCount=2 is the stable default. Large LLM model loading takes
  # 2-5 minutes, and scaling down before a new pod is ready causes a restart loop.
  # For autoscaling demo (scale-up observation), temporarily set minReplicaCount=1.
  minReplicaCount: 2    # Stable: prevent scale-down restart loop
  maxReplicaCount: 3    # Limited by GPU nodes (3× g6.4xlarge, 1 used by INT4 vLLM)
  
  # ═══════════════════════════════════════════════════════════════════════════
  # Polling and cooldown
  # ═══════════════════════════════════════════════════════════════════════════
  # IMPORTANT: For LLM workloads, cooldownPeriod must be LONGER than model
  # loading time. A 24B INT4 model takes 3-5+ minutes to load into GPU memory.
  # If cooldownPeriod is too short, KEDA will kill pods before they're ready.
  pollingInterval: 15     # Check metrics every 15s
  cooldownPeriod: 600     # Wait 10min before scaling down (model loading is slow!)
  
  # Scaling behavior
  advanced:
    horizontalPodAutoscalerConfig:
      behavior:
        scaleDown:
          # Long stabilization prevents thrashing during model loading
          stabilizationWindowSeconds: 600  # 10min stabilization for LLM
          policies:
            - type: Pods
              value: 1
              periodSeconds: 120  # Scale down 1 pod per 2 minutes max
        scaleUp:
          stabilizationWindowSeconds: 60   # 1min to scale up
          policies:
            - type: Pods
              value: 1
              periodSeconds: 30
  
  triggers:
    # ═══════════════════════════════════════════════════════════════════════════
    # PRIMARY TRIGGER: Queue Depth (vllm:num_requests_waiting)
    # ═══════════════════════════════════════════════════════════════════════════
    # This is the most effective metric for llm-d per Red Hat Engineering.
    # It represents users stuck in queue because KV Cache is full.
    # Target: Scale up when average queue length across shards > 5
    - type: prometheus
      metadata:
        # Thanos Querier endpoint (port 9092 per OCP 4.20 docs)
        serverAddress: https://thanos-querier.openshift-monitoring.svc.cluster.local:9092
        metricName: vllm_queue_depth
        namespace: private-ai
        # Average queue depth across all model replicas
        query: |
          sum(vllm:num_requests_waiting{namespace="private-ai"}) / count(vllm:num_requests_waiting{namespace="private-ai"})
        threshold: "5"  # Scale up when avg queue > 5
        ignoreNullValues: "true"
        unsafeSsl: "true"
        authModes: "bearer"
      authenticationRef:
        name: keda-trigger-auth-prometheus
    
    # ═══════════════════════════════════════════════════════════════════════════
    # SECONDARY TRIGGER: Inter-Token Latency (ITL) - P50
    # ═══════════════════════════════════════════════════════════════════════════
    # ITL measures decode phase latency - the user-perceived "streaming speed".
    # Target: 75ms median ITL per Red Hat performance validation.
    - type: prometheus
      metadata:
        serverAddress: https://thanos-querier.openshift-monitoring.svc.cluster.local:9092
        metricName: vllm_itl_p50_ms
        namespace: private-ai
        # P50 ITL in milliseconds
        query: |
          histogram_quantile(0.50,
            sum(rate(vllm:time_per_output_token_seconds_bucket{namespace="private-ai"}[2m])) by (le)
          ) * 1000
        threshold: "75"  # Scale up when median ITL > 75ms
        ignoreNullValues: "true"
        unsafeSsl: "true"
        authModes: "bearer"
      authenticationRef:
        name: keda-trigger-auth-prometheus

