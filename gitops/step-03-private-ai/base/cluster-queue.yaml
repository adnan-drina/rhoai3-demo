# ClusterQueue - Cluster-wide GPU quota pool
# Defines total GPU/CPU/Memory available for RHOAI workloads
#
# Quota Strategy (GPU-as-a-Service Demo):
#   - 2x g6.4xlarge (1 GPU each) = 2 GPUs for inference workloads
#   - 2x g6.12xlarge (4 GPUs each) = 8 GPUs for tensor-parallel models
#   - Total: 10 GPUs available for the private-ai project
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: rhoai-main-queue
  annotations:
    argocd.argoproj.io/sync-wave: "4"
spec:
  # Allow any namespace with a LocalQueue to use this pool
  namespaceSelector: {}
  
  # Preemption policy - what happens when quota is exceeded
  preemption:
    reclaimWithinCohort: Any
    withinClusterQueue: LowerPriority
  
  # Resource groups define quota limits per flavor
  resourceGroups:
    # L4 1GPU nodes (g6.4xlarge) - 2 nodes available
    - coveredResources: ["nvidia.com/gpu", "cpu", "memory"]
      flavors:
        - name: nvidia-l4-1gpu
          resources:
            - name: "nvidia.com/gpu"
              nominalQuota: 2       # 2 GPUs from 2x g6.4xlarge nodes
            - name: "cpu"
              nominalQuota: "32"    # 32 vCPU total (2x16)
            - name: "memory"
              nominalQuota: "128Gi" # 128GB RAM total (2x64)
    
    # L4 4GPU nodes (g6.12xlarge) - 2 nodes available
    - coveredResources: ["nvidia.com/gpu", "cpu", "memory"]
      flavors:
        - name: nvidia-l4-4gpu
          resources:
            - name: "nvidia.com/gpu"
              nominalQuota: 8        # 8 GPUs from 2x g6.12xlarge nodes
            - name: "cpu"
              nominalQuota: "96"     # 96 vCPU total (2x48)
            - name: "memory"
              nominalQuota: "384Gi"  # 384GB RAM total (2x192)

