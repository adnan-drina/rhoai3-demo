# ClusterQueue - Cluster-wide GPU quota pool
# Defines total GPU/CPU/Memory available for RHOAI workloads
#
# Quota Strategy (GPU-as-a-Service Demo):
#   - 1x g6.4xlarge (1 GPU) = 1 GPU for single-GPU inference
#   - 1x g6.12xlarge (4 GPUs) = 4 GPUs for tensor-parallel models
#   - Total: 5 GPUs available for the private-ai project
#
# IMPORTANT: Both flavors must be in the SAME resourceGroup so Kueue can
# try both flavors when matching workloads. A 4-GPU request should only
# match nvidia-l4-4gpu, not fail against nvidia-l4-1gpu first.
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: rhoai-main-queue
  annotations:
    argocd.argoproj.io/sync-wave: "4"
spec:
  # Allow any namespace with a LocalQueue to use this pool
  namespaceSelector: {}
  
  # Preemption policy - what happens when quota is exceeded
  preemption:
    reclaimWithinCohort: Any
    withinClusterQueue: LowerPriority
  
  # Flavor fungibility - try next flavor if first doesn't fit
  flavorFungibility:
    whenCanBorrow: Borrow
    whenCanPreempt: TryNextFlavor
  
  # SINGLE resourceGroup with BOTH flavors - critical for heterogeneous GPUs
  # Kueue will try nvidia-l4-1gpu first, then nvidia-l4-4gpu if request > 1 GPU
  resourceGroups:
    - coveredResources: ["nvidia.com/gpu", "cpu", "memory"]
      flavors:
        # 1-GPU flavor (g6.4xlarge nodes)
        - name: nvidia-l4-1gpu
          resources:
            - name: "nvidia.com/gpu"
              nominalQuota: 1       # 1 GPU from g6.4xlarge
            - name: "cpu"
              nominalQuota: "16"    # 16 vCPU
            - name: "memory"
              nominalQuota: "64Gi"  # 64GB RAM
        # 4-GPU flavor (g6.12xlarge nodes)
        - name: nvidia-l4-4gpu
          resources:
            - name: "nvidia.com/gpu"
              nominalQuota: 4       # 4 GPUs from g6.12xlarge
            - name: "cpu"
              nominalQuota: "48"    # 48 vCPU
            - name: "memory"
              nominalQuota: "192Gi" # 192GB RAM

