# ClusterQueue for llm-d Distributed Inference
# 
# This ClusterQueue provides a HARD RESERVATION of 2× g6.4xlarge GPUs
# exclusively for llm-d workloads, ensuring they can always start
# even when the main vLLM queue is saturated.
#
# Design Decision:
#   - Step 05/07 (vLLM): Uses rhoai-main-queue (1× g6.4xlarge + 1× g6.12xlarge = 5 GPUs)
#   - Step 08 (llm-d):   Uses rhoai-llmd-queue (2× g6.4xlarge = 2 GPUs reserved)
#
# GPU Node Allocation (total 3× g6.4xlarge + 1× g6.12xlarge):
#   ┌─────────────────────────────────────────────────────────────────────────┐
#   │  g6.12xlarge (4 GPUs)  │  Mistral-3 BF16 vLLM (Step 05)                │
#   ├─────────────────────────────────────────────────────────────────────────┤
#   │  g6.4xlarge #1 (1 GPU) │  Mistral-3 INT4 vLLM (Step 05)                │
#   ├─────────────────────────────────────────────────────────────────────────┤
#   │  g6.4xlarge #2 (1 GPU) │  llm-d Worker 0 (Step 08) ← RESERVED          │
#   ├─────────────────────────────────────────────────────────────────────────┤
#   │  g6.4xlarge #3 (1 GPU) │  llm-d Worker 1 (Step 08) ← RESERVED          │
#   └─────────────────────────────────────────────────────────────────────────┘
#
# Ref: https://kueue.sigs.k8s.io/docs/concepts/cluster_queue/
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: rhoai-llmd-queue
  labels:
    app.kubernetes.io/name: kueue
    app.kubernetes.io/component: cluster-queue
    app.kubernetes.io/part-of: private-ai
    demo.rhoai.io/step: "03"
  annotations:
    argocd.argoproj.io/sync-wave: "3"
    openshift.io/display-name: "llm-d Reserved GPU Queue"
    openshift.io/description: "Hard reservation of 2× g6.4xlarge GPUs for distributed inference"
spec:
  # Namespace selector - only accept workloads from private-ai
  namespaceSelector:
    matchLabels:
      kubernetes.io/metadata.name: private-ai
  
  # Preemption policy - llm-d workloads can preempt lower priority work
  preemption:
    reclaimWithinCohort: Any
    withinClusterQueue: LowerPriority
  
  # Resource quota - 2× nvidia-l4-1gpu (g6.4xlarge) reserved for llm-d
  resourceGroups:
    - coveredResources:
        - nvidia.com/gpu
        - cpu
        - memory
      flavors:
        - name: nvidia-l4-1gpu
          resources:
            # 2 GPUs total (1 per g6.4xlarge node, need 2 for tensor=2)
            - name: nvidia.com/gpu
              nominalQuota: 2
            # 2× g6.4xlarge = 32 vCPUs total (16 per node)
            - name: cpu
              nominalQuota: "32"
            # 2× g6.4xlarge = 128Gi memory total (64Gi per node)
            - name: memory
              nominalQuota: 128Gi
