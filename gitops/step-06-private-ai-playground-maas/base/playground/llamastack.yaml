# =============================================================================
# LlamaStackDistribution - GenAI Playground Backend
# =============================================================================
# The LlamaStackDistribution CR orchestrates the GenAI Playground backend.
# It translates UI actions into vLLM API calls and manages:
#   - Model inference routing
#   - RAG (vector_io with Milvus)
#   - MCP servers for tool calling
#   - Session/agent state management
#
# STATUS: Technology Preview (TP) in RHOAI 3.0
# Ref: https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0/html-single/experimenting_with_models_in_the_gen_ai_playground/index
#
# CRD Verification:
#   oc get crd llamastackdistributions.llamastack.io
#   oc explain llamastackdistribution.spec
#
# =============================================================================
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: lsd-genai-playground
  namespace: private-ai
  labels:
    opendatahub.io/dashboard: "true"
    app.kubernetes.io/name: llamastack
    app.kubernetes.io/part-of: private-ai-platform
    app.kubernetes.io/component: playground-backend
  annotations:
    argocd.argoproj.io/sync-wave: "5"
    argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true
    openshift.io/display-name: lsd-genai-playground
spec:
  replicas: 1
  server:
    # Distribution: Red Hat's pre-built LlamaStack distribution
    distribution:
      name: rh-dev
    
    # Container configuration
    containerSpec:
      name: llama-stack
      port: 8321
      command:
        - /bin/sh
        - -c
        - llama stack run /etc/llama-stack/run.yaml
      env:
        # TLS verification disabled for internal cluster communication
        - name: VLLM_TLS_VERIFY
          value: "false"
        # Milvus database path for RAG vector storage
        - name: MILVUS_DB_PATH
          value: ~/.llama/milvus.db
        # FMS orchestrator (not used in basic setup)
        - name: FMS_ORCHESTRATOR_URL
          value: http://localhost
        # Max tokens sent to vLLM (must be < model's --max-model-len)
        - name: VLLM_MAX_TOKENS
          value: "4096"
        # API tokens for vLLM authentication (not enforced in this demo)
        - name: VLLM_API_TOKEN_1
          value: fake
        - name: VLLM_API_TOKEN_2
          value: fake
        - name: VLLM_API_TOKEN_3
          value: fake
        - name: VLLM_API_TOKEN_4
          value: fake
        - name: VLLM_API_TOKEN_5
          value: fake
        # Config directory for LlamaStack
        - name: LLAMA_STACK_CONFIG_DIR
          value: /opt/app-root/src/.llama/distributions/rh/
      resources:
        requests:
          cpu: 250m
          memory: 500Mi
        limits:
          cpu: "2"
          memory: 12Gi
    
    # User-provided configuration (run.yaml)
    userConfig:
      configMapName: llama-stack-config
---
# =============================================================================
# LlamaStack Configuration (run.yaml)
# =============================================================================
# This ConfigMap contains the LlamaStack run.yaml that defines:
#   - Inference providers (vLLM endpoints for all 5 models)
#   - Vector I/O providers (Milvus for RAG)
#   - Tool runtimes (RAG, MCP)
#   - Model registrations
#
# All 5 models are registered even if not running (minReplicas: 0).
# LlamaStack will return errors for unavailable models, which is expected.
# =============================================================================
apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-stack-config
  namespace: private-ai
  labels:
    llamastack.io/distribution: lsd-genai-playground
    opendatahub.io/dashboard: "true"
  annotations:
    argocd.argoproj.io/sync-wave: "4"
    argocd.argoproj.io/sync-options: Replace=true
data:
  run.yaml: |
    # Llama Stack Configuration - All 5 Models
    version: "2"
    image_name: rh
    apis:
    - agents
    - datasetio
    - files
    - inference
    - safety
    - scoring
    - tool_runtime
    - vector_io
    providers:
      inference:
      # ═══════════════════════════════════════════════════════════════════════════
      # Embedding Model (for RAG)
      # ═══════════════════════════════════════════════════════════════════════════
      - provider_id: sentence-transformers
        provider_type: inline::sentence-transformers
        config: {}
      
      # ═══════════════════════════════════════════════════════════════════════════
      # vLLM Provider 1: Mistral-3 INT4 (1-GPU) - Efficient Chat
      # ═══════════════════════════════════════════════════════════════════════════
      - provider_id: vllm-mistral-int4
        provider_type: remote::vllm
        config:
          api_token: ${env.VLLM_API_TOKEN_1:=fake}
          max_tokens: ${env.VLLM_MAX_TOKENS:=4096}
          tls_verify: ${env.VLLM_TLS_VERIFY:=true}
          url: http://mistral-3-int4-predictor.private-ai.svc.cluster.local:8080/v1
      
      # ═══════════════════════════════════════════════════════════════════════════
      # vLLM Provider 2: Mistral-3 BF16 (4-GPU) - Full Precision
      # ═══════════════════════════════════════════════════════════════════════════
      - provider_id: vllm-mistral-bf16
        provider_type: remote::vllm
        config:
          api_token: ${env.VLLM_API_TOKEN_2:=fake}
          max_tokens: ${env.VLLM_MAX_TOKENS:=4096}
          tls_verify: ${env.VLLM_TLS_VERIFY:=true}
          url: http://mistral-3-bf16-predictor.private-ai.svc.cluster.local:8080/v1
      
      # ═══════════════════════════════════════════════════════════════════════════
      # vLLM Provider 3: Devstral-2 (4-GPU) - Agentic Coding
      # ═══════════════════════════════════════════════════════════════════════════
      - provider_id: vllm-devstral
        provider_type: remote::vllm
        config:
          api_token: ${env.VLLM_API_TOKEN_3:=fake}
          max_tokens: ${env.VLLM_MAX_TOKENS:=4096}
          tls_verify: ${env.VLLM_TLS_VERIFY:=true}
          url: http://devstral-2-predictor.private-ai.svc.cluster.local:8080/v1
      
      # ═══════════════════════════════════════════════════════════════════════════
      # vLLM Provider 4: GPT-OSS-20B (4-GPU) - Complex Reasoning
      # ═══════════════════════════════════════════════════════════════════════════
      - provider_id: vllm-gpt-oss
        provider_type: remote::vllm
        config:
          api_token: ${env.VLLM_API_TOKEN_4:=fake}
          max_tokens: ${env.VLLM_MAX_TOKENS:=4096}
          tls_verify: ${env.VLLM_TLS_VERIFY:=true}
          url: http://gpt-oss-20b-predictor.private-ai.svc.cluster.local:8080/v1
      
      # ═══════════════════════════════════════════════════════════════════════════
      # vLLM Provider 5: Granite-8B Agent (1-GPU) - Tool Calling
      # ═══════════════════════════════════════════════════════════════════════════
      - provider_id: vllm-granite-agent
        provider_type: remote::vllm
        config:
          api_token: ${env.VLLM_API_TOKEN_5:=fake}
          max_tokens: ${env.VLLM_MAX_TOKENS:=4096}
          tls_verify: ${env.VLLM_TLS_VERIFY:=true}
          url: http://granite-8b-agent-predictor.private-ai.svc.cluster.local:8080/v1
      
      vector_io:
      - provider_id: milvus
        provider_type: inline::milvus
        config:
          db_path: /opt/app-root/src/.llama/distributions/rh/milvus.db
          kvstore:
            db_path: /opt/app-root/src/.llama/distributions/rh/milvus_registry.db
            namespace: null
            type: sqlite
      agents:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          persistence_store:
            db_path: /opt/app-root/src/.llama/distributions/rh/agents_store.db
            namespace: null
            type: sqlite
          responses_store:
            db_path: /opt/app-root/src/.llama/distributions/rh/responses_store.db
            type: sqlite
      eval: []
      files:
      - provider_id: meta-reference-files
        provider_type: inline::localfs
        config:
          metadata_store:
            db_path: /opt/app-root/src/.llama/distributions/rh/files_metadata.db
            type: sqlite
          storage_dir: /opt/app-root/src/.llama/distributions/rh/files
      datasetio:
      - provider_id: huggingface
        provider_type: remote::huggingface
        config:
          kvstore:
            db_path: /opt/app-root/src/.llama/distributions/rh/huggingface_datasetio.db
            namespace: null
            type: sqlite
      scoring:
      - provider_id: basic
        provider_type: inline::basic
        config: {}
      - provider_id: llm-as-judge
        provider_type: inline::llm-as-judge
        config: {}
      tool_runtime:
      - provider_id: rag-runtime
        provider_type: inline::rag-runtime
        config: {}
      - provider_id: model-context-protocol
        provider_type: remote::model-context-protocol
        config: {}
    metadata_store:
      type: sqlite
      db_path: /opt/app-root/src/.llama/distributions/rh/inference_store.db
    
    # ═══════════════════════════════════════════════════════════════════════════
    # Model Registrations (All 5 Models)
    # ═══════════════════════════════════════════════════════════════════════════
    models:
    # Embedding model for RAG
    - provider_id: sentence-transformers
      model_id: granite-embedding-125m
      provider_model_id: ibm-granite/granite-embedding-125m-english
      model_type: embedding
      metadata:
        embedding_dimension: 768
    
    # LLM 1: Mistral-3 INT4 (1-GPU) - Efficient Chat
    - provider_id: vllm-mistral-int4
      model_id: mistral-3-int4
      model_type: llm
      metadata:
        description: "Mistral Small 24B INT4 - Efficient 1-GPU inference"
        display_name: "Mistral-3 INT4 (1-GPU)"
        gpu_count: 1
        use_case: chat assistant
    
    # LLM 2: Mistral-3 BF16 (4-GPU) - Full Precision
    - provider_id: vllm-mistral-bf16
      model_id: mistral-3-bf16
      model_type: llm
      metadata:
        description: "Mistral Small 24B BF16 - Full precision enterprise chat"
        display_name: "Mistral-3 BF16 (4-GPU)"
        gpu_count: 4
        use_case: enterprise chat assistant
    
    # LLM 3: Devstral-2 (4-GPU) - Agentic Coding
    - provider_id: vllm-devstral
      model_id: devstral-2
      model_type: llm
      metadata:
        description: "Devstral-2 - Optimized for agentic coding and tool use"
        display_name: "Devstral-2 (4-GPU Agentic)"
        gpu_count: 4
        use_case: agentic coding assistant
    
    # LLM 4: GPT-OSS-20B (4-GPU) - Complex Reasoning
    - provider_id: vllm-gpt-oss
      model_id: gpt-oss-20b
      model_type: llm
      metadata:
        description: "GPT-OSS-20B - High-reasoning OpenAI alternative"
        display_name: "GPT-OSS-20B (4-GPU BF16)"
        gpu_count: 4
        use_case: complex reasoning
    
    # LLM 5: Granite-8B Agent (1-GPU) - Tool Calling
    - provider_id: vllm-granite-agent
      model_id: granite-8b-agent
      model_type: llm
      metadata:
        description: "Granite 3.1 8B - Small but mighty tool-calling specialist"
        display_name: "Granite-3.1-8B Agent (1-GPU FP8)"
        gpu_count: 1
        use_case: agentic tool-calling
    
    shields: []
    vector_dbs: []
    datasets: []
    scoring_fns: []
    benchmarks: []
    tool_groups:
    - toolgroup_id: builtin::rag
      provider_id: rag-runtime
    server:
      port: 8321
