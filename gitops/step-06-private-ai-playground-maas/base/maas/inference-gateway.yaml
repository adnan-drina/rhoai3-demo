# ═══════════════════════════════════════════════════════════════════════════════
# InferenceGateway CR - PLACEHOLDER (MaaS Developer Preview)
# ═══════════════════════════════════════════════════════════════════════════════
#
# Status: Developer Preview (DP) - NOT AVAILABLE in RHOAI 3.0 GA
# Action: Implement when MaaS reaches Technology Preview (TP)
#
# The InferenceGateway CRD (maas.opendatahub.io/v1alpha1) is NOT present in
# RHOAI 3.0 GA clusters. This file is a placeholder documenting the expected
# configuration for when MaaS becomes available.
#
# Current Verification (should return empty):
#   oc get crd | grep -iE "inferencegateway|maas"
#   oc api-resources | grep -i maas
#
# MaaS Infrastructure Requirements:
#   1. Red Hat Connectivity Link 1.2 operator
#   2. Red Hat Connectivity Link (RHCL) operator (ties Authorino + Limitador together)
#   3. LLM-D runtime (Distributed Inference Server, not vLLM)
#   4. MaaS deployment script from opendatahub-io/maas-billing
#
# References:
#   - Blog: https://developers.redhat.com/articles/2025/11/25/introducing-models-service-openshift-ai
#   - Docs: https://opendatahub-io.github.io/models-as-a-service/latest/
#   - Repo: https://github.com/opendatahub-io/maas-billing
#
# ═══════════════════════════════════════════════════════════════════════════════
#
# PLACEHOLDER MANIFEST - DO NOT APPLY
# Uncomment and update when MaaS reaches TP
#
# ---
# apiVersion: maas.opendatahub.io/v1alpha1   # Verify with: oc explain inferencegateway
# kind: InferenceGateway
# metadata:
#   name: enterprise-gateway
#   namespace: private-ai
#   labels:
#     app.kubernetes.io/name: inference-gateway
#     app.kubernetes.io/part-of: private-ai-platform
#     app.kubernetes.io/component: maas-gateway
#   annotations:
#     argocd.argoproj.io/sync-wave: "10"
#     argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true
# spec:
#   # ═══════════════════════════════════════════════════════════════════════════
#   # Verify actual spec fields with: oc explain inferencegateway.spec
#   # ═══════════════════════════════════════════════════════════════════════════
#   
#   # Authentication - API key validation via Authorino
#   auth:
#     type: authorino
#     config:
#       apiKeySecretRef:
#         name: maas-api-keys
#         namespace: private-ai
#
#   # Rate Limiting - Request throttling via Limitador
#   rateLimit:
#     type: limitador
#     config:
#       limits:
#         - tier: free
#           requests: 5
#           tokens: 100
#           period: 60s
#         - tier: premium
#           requests: 100
#           tokens: 10000
#           period: 60s
#
#   # Model discovery - Auto-enroll InferenceServices by label
#   modelDiscovery:
#     enabled: true
#     selector:
#       matchLabels:
#         maas.opendatahub.io/enabled: "true"
#
# ═══════════════════════════════════════════════════════════════════════════════
# ALTERNATIVE: Manual Gateway API Configuration
# ═══════════════════════════════════════════════════════════════════════════════
#
# If InferenceGateway CRD is not available but you need a unified endpoint,
# you can manually configure:
#   1. Gateway (Kubernetes Gateway API)
#   2. HTTPRoute per model
#   3. AuthPolicy (Authorino)
#   4. RateLimitPolicy (Limitador)
#
# See: gateway-api-alternative.yaml for this approach
#
