# Step 05: LLM Inference with vLLM

> **Status**: ğŸš§ Placeholder - Implementation pending

Deploys the **Granite 3.1 8B Instruct FP8** model registered in Step 04 to a KServe inference endpoint using vLLM.

---

## Overview

This step completes the model deployment workflow:

```
Step 04: Model Registry          Step 05: Inference
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•         â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                                 
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Granite 3.1 FP8    â”‚          â”‚  KServe Endpoint    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚          â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Status: Registered â”‚ â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚  Status: Serving    â”‚
â”‚  URI: s3://...      â”‚          â”‚  URL: https://...   â”‚
â”‚  Ready to Deploy    â”‚          â”‚  GPU: NVIDIA L4     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          private-ai namespace                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚
â”‚   â”‚   InferenceService                                                      â”‚
â”‚   â”‚   granite-3-1-8b-instruct                                              â”‚
â”‚   â”‚                                                                         â”‚
â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚   â”‚   â”‚                    Predictor Pod                                â”‚  â”‚
â”‚   â”‚   â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚  â”‚
â”‚   â”‚   â”‚   â”‚   vLLM Engine   â”‚    â”‚   Model Weights â”‚                   â”‚  â”‚
â”‚   â”‚   â”‚   â”‚   OpenAI API    â”‚â—€â”€â”€â”€â”‚   From MinIO    â”‚                   â”‚  â”‚
â”‚   â”‚   â”‚   â”‚   :8000         â”‚    â”‚   FP8 Dynamic   â”‚                   â”‚  â”‚
â”‚   â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚  â”‚
â”‚   â”‚   â”‚                              â”‚                                  â”‚  â”‚
â”‚   â”‚   â”‚                              â–¼                                  â”‚  â”‚
â”‚   â”‚   â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚  â”‚
â”‚   â”‚   â”‚                    â”‚   NVIDIA L4     â”‚                          â”‚  â”‚
â”‚   â”‚   â”‚                    â”‚   16GB VRAM     â”‚                          â”‚  â”‚
â”‚   â”‚   â”‚                    â”‚   Kueue-managed â”‚                          â”‚  â”‚
â”‚   â”‚   â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚  â”‚
â”‚   â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚
â”‚                                                                             â”‚
â”‚   Exposed via: OpenShift Route (HTTPS)                                     â”‚
â”‚   API: OpenAI-compatible (/v1/chat/completions)                            â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Components

### ServingRuntime: vLLM

| Property | Value |
|----------|-------|
| **Name** | `vllm-runtime` |
| **Engine** | vLLM |
| **API** | OpenAI-compatible |
| **Port** | 8000 |

### InferenceService: Granite

| Property | Value |
|----------|-------|
| **Name** | `granite-3-1-8b-instruct` |
| **Model** | Granite 3.1 8B Instruct FP8 |
| **Source** | Model Registry (Step 04) |
| **GPU** | 1x NVIDIA L4 |
| **Quantization** | FP8-dynamic |

---

## Prerequisites

- [x] Step 01: GPU infrastructure with NVIDIA L4
- [x] Step 02: RHOAI 3.0 with KServe
- [x] Step 03: MinIO storage with model artifacts
- [x] Step 04: Model registered in registry

---

## Deploy

```bash
./steps/step-05-llm-on-vllm/deploy.sh
```

---

## Validation

```bash
# TODO: Add validation commands
# - Check InferenceService status
# - Test inference endpoint
# - Verify GPU allocation
```

---

## API Usage

```bash
# TODO: Add curl examples for OpenAI-compatible API
# POST /v1/chat/completions
```

---

## Documentation Links

- [Serving Models - RHOAI 3.0](https://docs.redhat.com/en/documentation/red_hat_openshift_ai_self-managed/3.0/html-single/serving_models/index)
- [vLLM Documentation](https://docs.vllm.ai/)
- [KServe Documentation](https://kserve.github.io/website/)

---

## TODO

- [ ] Implement vLLM ServingRuntime manifest
- [ ] Implement InferenceService manifest
- [ ] Configure model loading from MinIO
- [ ] Integrate with Kueue for GPU scheduling
- [ ] Add route/ingress configuration
- [ ] Implement deploy.sh script
- [ ] Add validation commands
- [ ] Document API usage examples

